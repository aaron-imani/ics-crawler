<!DOCTYPE html>
<html lang="en-US">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width" />
<title>AIML | Center for Machine Learning and Intelligent Systems</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="https://cml.ics.uci.edu/xmlrpc.php" />
<!--[if lt IE 9]>
<script src="https://cml.ics.uci.edu/wp-content/themes/bonpress-wpcom/js/html5.js" type="text/javascript"></script>
<![endif]-->

<meta name='robots' content='max-image-preview:large' />
<link rel='dns-prefetch' href='//s.w.org' />
<link rel="alternate" type="application/rss+xml" title="Center for Machine Learning and Intelligent Systems &raquo; Feed" href="https://cml.ics.uci.edu/feed/" />
<link rel="alternate" type="application/rss+xml" title="Center for Machine Learning and Intelligent Systems &raquo; Comments Feed" href="https://cml.ics.uci.edu/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Center for Machine Learning and Intelligent Systems &raquo; AIML Category Feed" href="https://cml.ics.uci.edu/category/aiml/feed/" />
<script type="text/javascript">
window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/14.0.0\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/14.0.0\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/cml.ics.uci.edu\/wp-includes\/js\/wp-emoji-release.min.js?ver=6.0.2"}};
/*! This file is auto-generated */
!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode,e=(p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0),i.toDataURL());return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([129777,127995,8205,129778,127999],[129777,127995,8203,129778,127999])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(e=t.source||{}).concatemoji?c(e.concatemoji):e.wpemoji&&e.twemoji&&(c(e.twemoji),c(e.wpemoji)))}(window,document,window._wpemojiSettings);
</script>
<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 0.07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='wp-block-library-css'  href='https://cml.ics.uci.edu/wp-includes/css/dist/block-library/style.min.css?ver=6.0.2' type='text/css' media='all' />
<style id='global-styles-inline-css' type='text/css'>
body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--duotone--dark-grayscale: url('#wp-duotone-dark-grayscale');--wp--preset--duotone--grayscale: url('#wp-duotone-grayscale');--wp--preset--duotone--purple-yellow: url('#wp-duotone-purple-yellow');--wp--preset--duotone--blue-red: url('#wp-duotone-blue-red');--wp--preset--duotone--midnight: url('#wp-duotone-midnight');--wp--preset--duotone--magenta-yellow: url('#wp-duotone-magenta-yellow');--wp--preset--duotone--purple-green: url('#wp-duotone-purple-green');--wp--preset--duotone--blue-orange: url('#wp-duotone-blue-orange');--wp--preset--font-size--small: 13px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--x-large: 42px;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-x-large-font-size{font-size: var(--wp--preset--font-size--x-large) !important;}
</style>
<link rel='stylesheet' id='bonpress-style-css'  href='https://cml.ics.uci.edu/wp-content/themes/bonpress-cml/style.css?ver=6.0.2' type='text/css' media='all' />
<link rel='stylesheet' id='tipsy-css'  href='https://cml.ics.uci.edu/wp-content/plugins/wp-shortcode/css/tipsy.css?ver=6.0.2' type='text/css' media='all' />
<link rel='stylesheet' id='mts_wpshortcodes-css'  href='https://cml.ics.uci.edu/wp-content/plugins/wp-shortcode/css/wp-shortcode.css?ver=6.0.2' type='text/css' media='all' />
<script type='text/javascript' src='https://cml.ics.uci.edu/wp-includes/js/jquery/jquery.min.js?ver=3.6.0' id='jquery-core-js'></script>
<script type='text/javascript' src='https://cml.ics.uci.edu/wp-includes/js/jquery/jquery-migrate.min.js?ver=3.3.2' id='jquery-migrate-js'></script>
<script type='text/javascript' src='https://cml.ics.uci.edu/wp-content/plugins/wp-shortcode/js/jquery.tipsy.js?ver=6.0.2' id='tipsy-js'></script>
<script type='text/javascript' src='https://cml.ics.uci.edu/wp-content/plugins/wp-shortcode/js/wp-shortcode.js?ver=6.0.2' id='mts_wpshortcodes-js'></script>
<link rel="https://api.w.org/" href="https://cml.ics.uci.edu/wp-json/" /><link rel="alternate" type="application/json" href="https://cml.ics.uci.edu/wp-json/wp/v2/categories/5" /><link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://cml.ics.uci.edu/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://cml.ics.uci.edu/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress 6.0.2" />
</head>

<body class="archive category category-aiml category-5 group-blog">
<div id="page" class="hfeed site">
		<header id="masthead" class="all-header" role="banner">
		<hgroup class="hgroup-wide">
                        <a href="https://cml.ics.uci.edu/" title="Center for Machine Learning and Intelligent Systems" rel="home"><img src='/wp-content/uploads/cml-curve.jpg'></a>
<!--			<h1 class="site-title"><a href="https://cml.ics.uci.edu/" title="Center for Machine Learning and Intelligent Systems" rel="home">Center for Machine Learning and Intelligent Systems</a></h1>
			<h2 class="site-description">University of California, Irvine</h2> -->
			<h1 class="site-title"><a href="https://cml.ics.uci.edu/" title="Center for Machine Learning and Intelligent Systems" rel="home">Center for Machine Learning and Intelligent Systems</a></h1>
			<h2 class="site-description">Bren School of Information and Computer Science</h2>			
			<h2 class="site-description">University of California, Irvine</h2>
			<div style="clear:both"></div>
		</hgroup>
	</header>
	<header id="masthead" class="site-header" role="banner">
		<hgroup class="hgroup-img">
                        <a href="https://cml.ics.uci.edu/" title="Center for Machine Learning and Intelligent Systems" rel="home"><img src='/wp-content/uploads/cml-curve.jpg'></a>
			<h1 class="site-title"><a href="https://cml.ics.uci.edu/" title="Center for Machine Learning and Intelligent Systems" rel="home">Center for Machine Learning and Intelligent Systems</a></h1>
			<h2 class="site-description">University of California, Irvine</h2>
		</hgroup>

		<nav id="site-navigation" class="navigation-main" role="navigation">
			<h1 class="menu-toggle">Menu</h1>
			<div class="screen-reader-text skip-link"><a href="#content" title="Skip to content">Skip to content</a></div>

			<div class="menu-navigation-container"><ul id="menu-navigation" class="menu"><li id="menu-item-234" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-234"><a href="https://cml.ics.uci.edu/">Home</a></li>
<li id="menu-item-79" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-79"><a href="https://cml.ics.uci.edu/home/about-us/">About CML</a>
<ul class="sub-menu">
	<li id="menu-item-78" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-78"><a href="https://cml.ics.uci.edu/home/about-us/">About us</a></li>
	<li id="menu-item-429" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-429"><a href="https://cml.ics.uci.edu/category/news/">News</a></li>
	<li id="menu-item-76" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-76"><a href="https://cml.ics.uci.edu/home/contact-us/">Contact Us</a></li>
</ul>
</li>
<li id="menu-item-539" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-539"><a>People</a>
<ul class="sub-menu">
	<li id="menu-item-55" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-55"><a href="https://cml.ics.uci.edu/faculty/">Faculty</a></li>
	<li id="menu-item-220" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-220"><a href="https://cml.ics.uci.edu/alumni/">Alumni</a></li>
</ul>
</li>
<li id="menu-item-75" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-75"><a href="https://cml.ics.uci.edu/aiml/">Events &#038; Seminars</a>
<ul class="sub-menu">
	<li id="menu-item-74" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-74"><a href="https://cml.ics.uci.edu/aiml/">AI/ML Seminar Series</a></li>
	<li id="menu-item-1090" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1090"><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">AI/ML Seminar Live Stream</a></li>
	<li id="menu-item-914" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-914"><a href="https://cml.ics.uci.edu/aiml/ml-distinguished-speakers/">CML Distinguished Speakers</a></li>
	<li id="menu-item-73" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-73"><a href="https://cml.ics.uci.edu/aiml/ml-reading-group/">ML Reading Group</a></li>
</ul>
</li>
<li id="menu-item-222" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-222"><a>Education &#038; Resources</a>
<ul class="sub-menu">
	<li id="menu-item-227" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-227"><a href="https://cml.ics.uci.edu/courses/">Courses</a></li>
	<li id="menu-item-221" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-221"><a href="https://cml.ics.uci.edu/books/">Books</a></li>
</ul>
</li>
<li id="menu-item-81" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-81"><a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI Machine Learning Archive</a></li>
<li id="menu-item-87" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-87"><a href="https://cml.ics.uci.edu/sponsors-funding/">Sponsors &#038; Funding</a></li>
<li id="menu-item-86" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-86"><a href="https://cml.ics.uci.edu/subscribe/">Subscribe to CML List</a></li>
</ul></div>		</nav><!-- #site-navigation -->
	</header><!-- #masthead -->


	<section id="primary" class="content-area">
		<div id="content" class="site-content" role="main">

		
			<header class="page-header clear">
				<h1 class="page-title">
					<span>AIML</span>				</h1>
							</header><!-- .page-header -->

						
				
<article id="post-1422" class="post-1422 post type-post status-publish format-standard hentry category-aiml">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2022/10/fall-2022/" title="Permalink to Fall 2022" rel="bookmark">Fall 2022</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		
<table class="wp-block-table aligncenter" cellpadding="5" border="1">
   <colgroup><col width="100"><col>
</colgroup><tbody>

  <tr>
  <td valign="top"><div class="aiml-date"><b>Oct. 10</b><br><b>DBH 4011</b><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="https://furong-huang.com/"><img src="http://cml.ics.uci.edu/wp-content/uploads/FurongHuang.jpeg" width="150px"><br><b>Furong Huang</b></a><br>Assistant Professor of Computer Science<br>University of Maryland</div><br>  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Trustworthy Machine Learning in Complex Environments</b></a></span></div><div class="togglec clearfix">With the burgeoning use of machine learning models in an assortment of applications, there is a need to rapidly and reliably deploy models in a variety of environments. These trustworthy machine learning models must satisfy certain criteria, namely the ability to: (i) adapt and generalize to previously unseen worlds although trained on data that only represent a subset of the world, (ii) allow for non-iid data, (iii) be resilient to (adversarial) perturbations, and (iv) conform to social norms and make ethical decisions.  In this talk, towards trustworthy and generally applicable intelligent systems, I will cover some reinforcement learning algorithms that achieve fast adaptation by guaranteed knowledge transfer, principled methods that measure the vulnerability and improve the robustness of reinforcement learning agents, and ethical models that make fair decisions under distribution shifts.
<br><br>
<b>Bio:</b> Furong Huang is an Assistant Professor of the Department of Computer Science at University of Maryland. She works on statistical and trustworthy machine learning, reinforcement learning, graph neural networks, deep learning theory and federated learning with specialization in domain adaptation, algorithmic robustness and fairness. Furong is a recipient of the NSF CRII Award, the MLconf Industry Impact Research Award, the Adobe Faculty Research Award, and three JP Morgan Faculty Research Awards. She is a Finalist of AI in Research &#8211; AI researcher of the year for Women in AI Awards North America 2022. She received her Ph.D. in electrical engineering and computer science from UC Irvine in 2016, after which she completed postdoctoral positions at Microsoft Research NYC.</div></div><div class="clear"></div>
  </td>
  </tr>

  <tr>
  <td valign="top"><div class="aiml-date"><b>Oct. 17</b><br><b>DBH 4011</b><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="https://www.majumderb.com/"><img src="http://cml.ics.uci.edu/wp-content/uploads/BodhiMajumder.jpeg" width="150px"><br><b>Bodhi Majumder</b></a><br>PhD Student, Department of Computer Science and Engineering<br>University of California, San Diego</div><br>  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Effective, explainable, and equitable predictions in NLP models with world knowledge and conversations</b></a></span></div><div class="togglec clearfix">The use of artificial intelligence in knowledge-seeking applications (e.g., for recommendations and explanations) has shown remarkable effectiveness. However, the increasing demand for more interactions, accessibility and user-friendliness in these systems requires the underlying components (dialog models, LLMs) to be adequately grounded in the up-to-date real-world context. However, in reality, even powerful generative models often lack commonsense, explanations, and subjectivity &#8212; a long-standing goal of artificial general intelligence.  In this talk, I will partly address these problems in three parts and hint at future possibilities and social impacts. Mainly, I will discuss: 1) methods to effectively inject up-to-date knowledge in an existing dialog model without any additional training, 2) the role of background knowledge in generating faithful natural language explanations, and 3) a conversational framework to address subjectivity—balancing task performance and bias mitigation for fair interpretable predictions. 
<br><br>
<b>Bio:</b> Bodhisattwa Prasad Majumder is a final-year PhD student at CSE, UC San Diego, advised by Prof. Julian McAuley. His research goal is to build interactive machines capable of producing knowledge grounded explanations. He previously interned at Allen Institute of AI, Google AI, Microsoft Research, FAIR (Meta AI) and collaborated with U of Oxford, U of British Columbia, and Alan Turing Institute. He is a recipient of the UCSD CSE Doctoral Award for Research (2022), Adobe Research Fellowship (2022), UCSD Friends Fellowship (2022), and Qualcomm Innovation Fellowship (2020). In 2019, Bodhi led UCSD in the finals of Amazon Alexa Prize. He also co-authored a best-selling NLP book with O’Reilly Media that is being adopted in universities internationally.  Website: <a href="http://www.majumderb.com/">http://www.majumderb.com/</a>.</div></div><div class="clear"></div>
  </td>
  </tr>    

  <tr>
  <td valign="top"><div class="aiml-date"><b>Oct. 24</b><br><b>DBH 4011</b><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="https://steyvers.socsci.uci.edu/"><img src="http://cml.ics.uci.edu/wp-content/uploads/MarkSteyvers.jpeg" width="150px"><br><b>Mark Steyvers</b></a><br>Professor of Cognitive Sciences<br>University of California, Irvine</div><br>  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Human-AI collaboration</b></a></span></div><div class="togglec clearfix">Artificial intelligence (AI) and machine learning models are being increasingly deployed in real-world applications. In many of these applications, there is strong motivation to develop hybrid systems in which humans and AI algorithms can work together, leveraging their complementary strengths and weaknesses. In the first part of the presentation, I will discuss results from a Bayesian framework where we statistically combine the predictions from humans and machines while taking into account the unique ways human and algorithmic confidence is expressed. The framework allows us to investigate the factors that influence complementarity, where a hybrid combination of human and machine predictions leads to better performance than combinations of human or machine predictions alone. In the second part of the presentation, I will discuss some recent work on AI-assisted decision making where individuals are presented with recommended predictions from classifiers. Using a cognitive modeling approach, we can estimate the AI reliance policy used by individual participants. The results  show that AI advice is more readily adopted if the individual is in a low confidence state, receives high-confidence advice from the AI and when the AI is generally more accurate. In the final part of the presentation, I will discuss the question of “machine theory of mind” and “theory of machine”, how humans and machines can efficiently form mental models of each other. I will show some recent results on theory-of-mind experiments where the goal is for individuals and machine algorithms to predict the performance of other individuals in image classification tasks. The results show performance gaps where human individuals outperform algorithms in mindreading tasks. I will discuss several research directions designed to close the gap. 
<br><br>
<b>Bio:</b> Mark Steyvers is a Professor of Cognitive Science at UC Irvine and Chancellor’s Fellow. He has a joint appointment with the  Computer Science department and is affiliated with the Center for Machine Learning and Intelligent Systems. His publications span work in cognitive science as well as machine learning and has been funded by NSF, NIH, IARPA, NAVY, and AFOSR. He received his PhD from Indiana University and was a Postdoctoral Fellow at Stanford University. He is currently serving as Associate Editor of Computational Brain and Behavior and Consulting Editor for Psychological Review and has previously served as the President of the Society of Mathematical Psychology, Associate Editor for Psychonomic Bulletin &amp; Review and the Journal of Mathematical Psychology. In addition, he has served as a consultant for a variety of companies such as eBay, Yahoo, Netflix, Merriam Webster, Rubicon and Gimbal on machine learning problems. Dr. Steyvers received New Investigator Awards from the American Psychological Association as well as the Society of Experimental Psychologists. He also received an award from the Future of Privacy Forum and Alfred P. Sloan Foundation for his collaborative work with Lumosity.</div></div><div class="clear"></div>
  </td>
  </tr>    

  <tr>
  <td valign="top"><div class="aiml-date"><b>Oct. 31</b><br><b>DBH 4011</b><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="https://scholar.google.com/citations?user=n7P_6uQAAAAJ&amp;hl=en"><img src="http://cml.ics.uci.edu/wp-content/uploads/AlexBoyd.jpeg" width="150px"><br><b>Alex Boyd</b></a><br>PhD Student, Department of Statistics<br>University of California, Irvine</div><br>  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>To Be Announced</b></a></span></div><div class="togglec clearfix">TBD. 
<br><br>
<b>Bio:</b> TBD.</div></div><div class="clear"></div>
  </td>
  </tr>    

  <tr>
  <td valign="top"><div class="aiml-date"><b>Nov. 7</b><br><b>DBH 4011</b><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="https://sites.google.com/uci.edu/yanning-shen/home"><img src="http://cml.ics.uci.edu/wp-content/uploads/YanningShen.jpeg" width="150px"><br><b>Yanning Shen</b></a><br>Assistant Professor of Electrical Engineering and Computer Science<br>University of California, Irvine</div><br>  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>To Be Announced</b></a></span></div><div class="togglec clearfix">TBD. 
<br><br>
<b>Bio:</b> TBD.</div></div><div class="clear"></div>
  </td>
  </tr>    

  <tr>
  <td valign="top"><div class="aiml-date"><b>Nov. 14</b><br><b>DBH 4011</b><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="https://muhaochen.github.io/"><img src="http://cml.ics.uci.edu/wp-content/uploads/MuhaoChen.jpeg" width="150px"><br><b>Muhao Chen</b></a><br>Assistant Research Professor of Computer Science<br>University of Southern California</div><br>  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>To Be Announced</b></a></span></div><div class="togglec clearfix">TBD. 
<br><br>
<b>Bio:</b> TBD.</div></div><div class="clear"></div>
  </td>
  </tr>    

  <tr>
  <td valign="top"><div class="aiml-date"><b>Nov. 21</b><br><b>DBH 4011</b><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><b>TBD</b></div><br>  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>To Be Announced</b></a></span></div><div class="togglec clearfix">TBD. 
<br><br>
<b>Bio:</b> TBD.</div></div><div class="clear"></div>
  </td>
  </tr>    

  <tr>
  <td class="aiml-none"><div class="aiml-date"><b>Nov. 28</b></div></td>
  <td class="aiml-none"><div class="aiml-name"><b>No Seminar (<a href="https://neurips.cc/">NeurIPS Conference</a>)</b></div>
  </td>
  </tr>

</tbody></table>



<p></p>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2022/10/fall-2022/" title="5:37 pm" rel="bookmark"><time class="entry-date genericon" datetime="2022-10-04T17:37:09-07:00">October 4, 2022</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
				
<article id="post-1369" class="post-1369 post type-post status-publish format-standard hentry category-aiml">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2022/04/spring-2022/" title="Permalink to Spring 2022" rel="bookmark">Spring 2022</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		
<h2><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream for all Spring 2022 CML Seminars</a></h2>

<table class="wp-block-table aligncenter" cellpadding="5" border="1">
   <colgroup><col width="100"><col>
</colgroup><tbody>

  <tr>
  <td valign="top"><div class="aiml-date"><b>May 2</b><br><b>DBH 4011</b> &amp;<br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="https://www.eurecom.fr/~filippon/index.html"><img src="https://www.eurecom.fr/~filippon/layout/photo.jpeg" width="150px"><br><b>Maurizio Filippone</b></a><br>Associate Professor, EURECOM<br>and<br><a href="https://tranbahien.github.io/"><b>Ba-Hien Tran</b></a><br>PhD Student, EURECOM<br><br>YouTube Stream: <a href="https://youtu.be/oZAuh686ipw">https://youtu.be/oZAuh686ipw</a></div><br>  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Functional Priors for Bayesian Deep Learning</b></a></span></div><div class="togglec clearfix">The Bayesian treatment of neural networks dictates that a prior distribution is specified over their weight and bias parameters. This poses a challenge because modern neural networks are characterized by a huge number of parameters and non-linearities. The choice of these priors has an unpredictable effect on the distribution of the functional output which could represent a hugely limiting aspect of Bayesian deep learning models. Differently, Gaussian processes offer a rigorous non-parametric framework to define prior distributions over the space of functions. In this talk, we aim to introduce a novel and robust framework to impose such functional priors on modern neural networks for supervised learning tasks through minimizing the Wasserstein distance between samples of stochastic processes. In addition, we extend this framework to carry out model selection for Bayesian autoencoders for unsupervised learning tasks. We provide extensive experimental evidence that coupling these priors with scalable Markov chain Monte Carlo sampling offers systematically large performance improvements over alternative choices of priors and state-of-the-art approximate Bayesian deep learning approaches.
<br><br>
<b>Bio:</b> Maurizio Filippone received a Master&#8217;s degree in Physics and a Ph.D. in Computer Science from the University of Genova, Italy, in 2004 and 2008, respectively. In 2007, he was a Research Scholar with George Mason University, Fairfax, VA. From 2008 to 2011, he was a Research Associate with the University of Sheffield, U.K. (2008-2009), with the University of Glasgow, U.K. (2010), and with University College London, U.K (2011). From 2011 to 2015 he was a Lecturer at the University of Glasgow, U.K, and he is currently AXA Chair of Computational Statistics and Associate Professor at EURECOM, Sophia Antipolis, France. His current research interests include the development of tractable and scalable Bayesian inference techniques for Gaussian processes and Deep/Conv Nets with applications in life and environmental sciences.<br>
<b>Bio:</b> Ba-Hien Tran is currently a PhD student within the Data Science department of EURECOM, under the supervision of Professor Maurizio Filippone. His research focuses on Accelerating Inference for Deep Probabilistic Modeling. In 2016, he received a Bachelor of Science degree with honors in Computer Science from Vietnam National University, HCMC. His thesis investigated Deep Learning approaches for data-driven image captioning. In 2020, he received a Master of Science in Engineering degree in Data Science from Télécom Paris. His thesis focused on Bayesian Inference for Deep Neural Networks.</div></div><div class="clear"></div>
  </td>
  </tr>

<tr>
  <td valign="top"><div class="aiml-date"><b>May 9</b><br><b>DBH 4011</b> &amp;<br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="http://www.tivaro.nl/"><img src="https://i1.rgstatic.net/ii/profile.image/858097014829060-1581597679382_Q128/Ties-Van-Rozendaal.jpg" width="150px"><br><b>Ties van Rozendaal</b></a><br>Senior Machine Learning Researcher<br>Qualcomm AI Research<br><br>YouTube Stream: <a href="https://youtu.be/LQu-kwpfFg4">https://youtu.be/LQu-kwpfFg4</a></div><br>  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Instance-adaptive data compression: Improving Neural Codecs by Training on the Test Set</b></a></span></div><div class="togglec clearfix">Neural data compression has been shown to outperform classical methods in terms of rate-distortion performance, with results still improving rapidly. These models are fitted to a training dataset and cannot be expected to optimally compress test data in general due to limitations on model capacity, distribution shifts, and imperfect optimization. If the test-time data distribution is known and has relatively low entropy, the model can easily be finetuned or adapted to this distribution. Instance-adaptive methods take this approach to the extreme, adapting the model to a single test instance, and signaling the updated model along in the bitstream. In this talk, we will show the potential of different types of instance-adaptive methods and discuss the tradeoffs that these methods pose. 
<br><br>
<b>Bio:</b> Ties is a senior machine learning researcher at Qualcomm AI Research. He obtained his masters’s degree at the University of Amsterdam with a thesis on personalizing automatic speech recognition systems using unsupervised methods. At Qualcomm AI research he has been working on neural compression, with a focus on using generative models to compress image and video data. His research includes work on semantic compression and constrained optimization as well as instance-adaptive and neural-implicit compression.</div></div><div class="clear"></div>
  </td>
  </tr>    

<tr>
  <td valign="top"><div class="aiml-date"><b>May 16</b><br><b>DBH 4011</b> &amp;<br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="https://robinjia.github.io/"><img src="https://robinjia.github.io/assets/images/profile.jpg" width="150px"><br><b>Robin Jia</b></a><br>Assistant Professor of Computer Science<br>University of Southern California<br><br>YouTube Stream: <a href="https://youtu.be/ALqqlgbzAB0">https://youtu.be/ALqqlgbzAB0</a></div><br>  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Out-of-Distribution Evaluation: The How, the Which, and the “What?!”</b></a></span></div><div class="togglec clearfix">Natural language processing (NLP) models have achieved impressive accuracies on in-distribution benchmarks, but they are unreliable in out-of-distribution (OOD) settings. In this talk, I will give an exclusive preview of my group’s ongoing work on evaluating and improving model performance in OOD settings. First, I will propose likelihood splits, a general-purpose way to create challenging non-i.i.d. benchmarks by measuring generalization to the tail of the data distribution, as identified by a language model. Second, I will describe the advantages of neurosymbolic approaches over end-to-end pretrained models for OOD generalization in visual question answering; these results highlight the importance of measuring OOD generalization when comparing modeling approaches. Finally, I will show how synthesized examples can improve open-set recognition, the task of abstaining on OOD examples that come from classes never seen at training time.
<br><br>
<b>Bio:</b> Robin Jia is an Assistant Professor of Computer Science at the University of Southern California. He received his Ph.D. in Computer Science from Stanford University, where he was advised by Percy Liang. He has also spent time as a visiting researcher at Facebook AI Research, working with Luke Zettlemoyer and Douwe Kiela. He is interested broadly in natural language processing and machine learning, with a particular focus on building NLP systems that are robust to distribution shift. Robin’s work has received best paper awards at ACL and EMNLP.</div></div><div class="clear"></div>
  </td>
  </tr>

  <tr>
  <td class="aiml-none"><div class="aiml-date"><b>May 23</b></div></td>
  <td class="aiml-none"><div class="aiml-name"><b>No Seminar</b></div>
  </td>
  </tr>

  <tr>
  <td class="aiml-none"><div class="aiml-date"><b>May 30</b></div></td>
  <td class="aiml-none"><div class="aiml-name"><b>No Seminar (Memorial Day Holiday)</b></div>
  </td>
  </tr>

<tr>
  <td valign="top"><div class="aiml-date"><b>June 6</b><br><b>DBH 4011</b> &amp;<br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="https://www.linkedin.com/in/bobak-pezeshki-9a545b156/"><img src="http://cml.ics.uci.edu/wp-content/uploads/BobakPezeshki.jpeg" width="150px"><br><b>Bobak Pezeshki</b></a><br>PhD Student, Department of Computer Science<br>University of California, Irvine<br><br>YouTube Stream: <a href="https://youtu.be/Yl_aCTieVqc">https://youtu.be/Yl_aCTieVqc</a></div><br>  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>AND/OR Branch-and-Bound for Computational Protein Design Optimizing K*</b></a></span></div><div class="togglec clearfix">Computational protein design (CPD) is the task of creating new proteins to fulfill a desired function.  In this talk, I will share work recently accepted at UAI 2022 based on a new formulation of CPD as a graphical model designed for optimizing subunit binding affinity.  These new methods showed promising results when compared with state-of-the-art algorithm BBK* that is part of a long-time developed software package dedicated to CPD.  In the talk, I will first describe CPD in general and for optimizing a quantity called K* (which approximates binding affinity).  I will relate this to the well known task of MMAP for which many powerful algorithms have been recently developed and from which our methods are inspired.  Next I will give a preview of the promising results of our new framework. I will then go on to describe the framework, presenting the formulation of the problem as a graphical model for K* optimization and introducing a weighted mini-bucket heuristic for bounding K* and guiding search. Finally, I will share our algorithm AOBB-K* and modifications that can enhance it, describing some of the empirical benefits and limitations of our scheme.  To conclude, I will outline some future directions for advancing the use of this framework.
<br><br>
<b>Bio:</b> Bobak Pezeshki is a fifth year PhD student of Computer Science at the University of California, Irvine, under advisement of Professor Rina Dechter.  His research focus is in automated reasoning over graphical models with focus in Abstraction Sampling and applying automated reasoning over graphical models to computational protein design.  He completed his undergraduate studies at UC Berkeley majoring in Molecular and Cell Biology (with an emphasis in Biochemistry) and Integrative Biology.  Before pursuing his PhD at UCI, he was involved in protein biochemistry research at the Stroud Lab, UCSF, and at Novartis Vaccines and Diagnostics.</div></div><div class="clear"></div>
  </td>
  </tr>

</tbody></table>



<p></p>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2022/04/spring-2022/" title="9:43 pm" rel="bookmark"><time class="entry-date genericon" datetime="2022-04-28T21:43:07-07:00">April 28, 2022</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
				
<article id="post-1297" class="post-1297 post type-post status-publish format-standard hentry category-aiml">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2022/01/winter-2022/" title="Permalink to Winter 2022" rel="bookmark">Winter 2022</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		
<h2><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream for all Winter 2022 CML Seminars</a></h2>

<table class="wp-block-table aligncenter" cellpadding="5" border="1">
   <colgroup><col width="100"><col>
</colgroup><tbody>

  <tr>
  <td class="aiml-none"><div class="aiml-date"><b>January 3</b></div></td>
  <td class="aiml-none"><div class="aiml-name"><b>No Seminar</b></div>
  </td>
  </tr>

  <tr>
  <td valign="top"><div class="aiml-date"><b>January 10</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="https://royf.org/"><img src="http://cml.ics.uci.edu/wp-content/uploads/fox.jpg" width="150px"><br><b>Roy Fox</b></a><br>Assistant Professor<br>Department of Computer Science<br>University of California, Irvine<br><br>YouTube Stream: <a href="https://youtu.be/ImvsK5CFp0w">https://youtu.be/ImvsK5CFp0w</a></div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Curiously effective ensemble and double-oracle reinforcement-learning methods</b></a></span></div><div class="togglec clearfix">Ensemble methods for reinforcement learning have gained attention in recent years, due to their ability to represent model uncertainty and use it to guide exploration and to reduce value estimation bias. We present MeanQ, a very simple ensemble method with improved performance, and show how it reduces estimation variance enough to operate without a stabilizing target network. Curiously, MeanQ is theoretically *almost* equivalent to a non-ensemble state-of-the-art method that it significantly outperforms, raising questions about the interaction between uncertainty estimation, representation, and resampling.
<br>
In adversarial environments, where a second agent attempts to minimize the first&#8217;s rewards, double-oracle (DO) methods grow a population of policies for both agents by iteratively adding the best response to the current population. DO algorithms are guaranteed to converge when they exhaust all policies, but are only effective when they find a small population sufficient to induce a good agent. We present XDO, a DO algorithm that exploits the game&#8217;s sequential structure to exponentially reduce the worst-case population size. Curiously, the small population size that XDO needs to find good agents more than compensates for its increased difficulty to iterate with a given population size.
<br><br>
<b>Bio:</b> Roy Fox is an Assistant Professor and director of the Intelligent Dynamics Lab at the Department of Computer Science at UCI. He was previously a postdoc in UC Berkeley&#8217;s BAIR, RISELab, and AUTOLAB, where he developed algorithms and systems that interact with humans to learn structured control policies for robotics and program synthesis. His research interests include theory and applications of reinforcement learning, algorithmic game theory, information theory, and robotics. His current research focuses on structure, exploration, and optimization in deep reinforcement learning and imitation learning of virtual and physical agents and multi-agent systems.</div></div><div class="clear"></div>
  </td>
  </tr>

  <tr>
  <td class="aiml-none"><div class="aiml-date"><b>January 17</b></div></td>
  <td class="aiml-none"><div class="aiml-name"><b>No Seminar (Martin Luther King, Jr. Day)</b></div>
  </td>
  </tr>

  <tr>
  <td valign="top"><div class="aiml-date"><b>January 24</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="https://www.ransalu.com/"><img src="http://cml.ics.uci.edu/wp-content/uploads/ransaluSenanayake.png" width="150px"><br><b>Ransalu Senanayake</b></a><br>Postdoctoral Scholar<br>Department of Computer Science<br>Stanford University<br><br>YouTube Stream: <a href="https://youtu.be/3yR8BqBElXw">https://youtu.be/3yR8BqBElXw</a></div><br>  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Propagating Uncertainty from Modeling into Decision-Making for Trustworthy Autonomy</b></a></span></div><div class="togglec clearfix">Autonomous agents such as self-driving cars have already gained the capability to perform individual tasks such as object detection and lane following, especially in simple, static environments. While advancing robots towards full autonomy, it is important to minimize deleterious effects on humans and infrastructure to ensure the trustworthiness of such systems. However, for robots to safely operate in the real world, it is vital for them to quantify the multimodal aleatoric and epistemic uncertainty around them and use that uncertainty for decision-making. In this talk, I will talk about how we can leverage tools from approximate Bayesian inference, kernel methods, and deep neural networks to develop interpretable autonomous systems for high-stakes applications.
<br><br>
<b>Bio:</b> Ransalu Senanayake is a postdoctoral scholar in the Statistical Machine Learning Group at the Department of Computer Science, Stanford University. He focuses on making downstream applications of machine learning trustworthy by quantifying uncertainty and explaining the decisions of such systems. Currently, he works with Prof. Emily Fox and Prof. Carlos Guestrin. He also worked on decision-making under uncertainty with Prof. Mykel Kochenderfer. Prior to joining Stanford, Ransalu obtained a PhD in Computer Science from the University of Sydney, Australia, and an MPhil in Industrial Engineering and Decision Analytics from the Hong Kong University of Science and Technology, Hong Kong.</div></div><div class="clear"></div>
  </td>
  </tr>

  <tr>
  <td valign="top"><div class="aiml-date"><b>January 31</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="https://dylanslacks.website/"><img src="http://cml.ics.uci.edu/wp-content/uploads/dylanSlack.png" width="150px"><br><b>Dylan Slack</b></a><br>PhD Student<br>Department of Computer Science<br>University of California, Irvine<br><br>YouTube Stream: <a href="https://youtu.be/71RJvjPhk3U">https://youtu.be/71RJvjPhk3U</a></div><br>  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Exposing Shortcomings and Improving the Reliability of Machine Learning Explanations</b></a></span></div><div class="togglec clearfix">For domain experts to adopt machine learning (ML) models in high-stakes settings such as health care and law, they must understand and trust model predictions. As a result, researchers have proposed numerous ways to explain the predictions of complex ML models. However, these approaches suffer from several critical drawbacks, such as vulnerability to adversarial attacks, instability, inconsistency, and lack of guidance about accuracy and correctness. For practitioners to safely use explanations in the real world, it is vital to properly characterize the limitations of current techniques and develop improved explainability methods. This talk will describe the shortcomings of explanations and introduce current research demonstrating how they are vulnerable to adversarial attacks. I will also discuss promising solutions and present recent work on explanations that leverage uncertainty estimates to overcome several critical explanation shortcomings.
<br><br>
<b>Bio:</b> Dylan Slack is a Ph.D. candidate at UC Irvine advised by Sameer Singh and Hima Lakkaraju and associated with UCI NLP, CREATE, and the HPI Research Center. His research focuses on developing techniques that help researchers and practitioners build more robust, reliable, and trustworthy machine learning models. In the past, he has held research internships at GoogleAI and Amazon AWS and was previously an undergraduate at Haverford College advised by Sorelle Friedler where he researched fairness in machine learning.</div></div><div class="clear"></div>
  </td>
  </tr>

  <tr>
  <td valign="top"><div class="aiml-date"><b>February 7</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="http://maja-rita-rudolph.com/"><img src="http://cml.ics.uci.edu/wp-content/uploads/majaRudolph.jpg" width="150px"><br><b>Maja Rudolph</b></a><br>Senior Research Scientist<br>Bosch Center for AI<br><br>YouTube Stream: <a href="https://youtu.be/9fRw74WhRdE">https://youtu.be/9fRw74WhRdE</a></div><br>  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Modeling Irregular Time Series with Continuous Recurrent Units</b></a></span></div><div class="togglec clearfix">Recurrent neural networks (RNNs) are a popular choice for modeling sequential data.  Standard RNNs assume constant time-intervals between observations. However, in many datasets (e.g. medical records) observation times are irregular and can carry important information.  To address this challenge, we propose continuous recurrent units (CRUs) &#8211; a neural architecture that can naturally handle irregular intervals between observations. The CRU assumes a hidden state which evolves according to a linear stochastic differential equation and is integrated into an encoder-decoder framework. The recursive computations of the CRU can be derived using the continuous-discrete Kalman filter and are in closed form. The resulting recurrent architecture has temporal continuity between hidden states and a gating mechanism that can optimally integrate noisy observations. We derive an efficient parametrization scheme for the CRU that leads to a fast implementation (f-CRU). We empirically study the CRU on a number of challenging datasets and find that it can interpolate irregular time series better than methods based on neural ordinary differential equations.
<br><br>
<b>Bio:</b> Maja Rudolph is a Senior Research Scientist at the Bosch Center for AI where she works on machine learning research questions derived from engineering problems: for example, how to model driving behavior, how to forecast the operating conditions of a device, or how to find anomalies in the sensor data of an assembly line. In 2018, Maja completed her Ph.D. in Computer Science at Columbia University, advised by David Blei. She holds a MS in Electrical Engineering from Columbia University and a BS in Mathematics from MIT.</div></div><div class="clear"></div>
  </td>
  </tr>

  <tr>
  <td valign="top"><div class="aiml-date"><b>February 14</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="https://ruiqigao.github.io/"><img src="http://cml.ics.uci.edu/wp-content/uploads/ruiqiGao.png" width="150px"><br><b>Ruiqi Gao</b></a><br>Research Scientist<br>Google Brain<br><br>YouTube Stream: <a href="https://youtu.be/eAozs_JKp4o">https://youtu.be/eAozs_JKp4o</a></div><br>  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Advanced training of energy-based models</b></a></span></div><div class="togglec clearfix">Energy-based models (EBMs) are an appealing class of probabilistic models, which can be viewed as generative versions of discriminators, yet can be learned from unlabeled data. Despite a number of desirable properties, two challenges remain for training EBMs on high-dimensional datasets. First, learning EBMs by maximum likelihood requires Markov Chain Monte Carlo (MCMC) to generate samples from the model, which can be extremely expensive. Second, the energy potentials learned with non-convergent MCMC can be highly biased, making it difficult to evaluate the learned energy potentials or apply the learned models to downstream tasks.<br>In this talk, I will present two algorithms to tackle the challenges of training EBMs. (1) Diffusion Recovery Likelihood, where we tractably learn and sample from a sequence of EBMs trained on increasingly noisy versions of a dataset. Each EBM is trained with recovery likelihood, which maximizes the conditional probability of the data at a certain noise level given their noisy versions at a higher noise level. (2) Flow Contrastive Estimation, where we jointly estimate an EBM and a flow-based model, in which the two models are iteratively updated based on a shared adversarial value function. We demonstrate that EBMs can be trained with a small budget of MCMC or completely without MCMC. The learned energy potentials are faithful and can be applied to likelihood evaluation and downstream tasks, such as feature learning and semi-supervised learning. 
<br><br>
<b>Bio:</b> Ruiqi Gao is a research scientist at Google, Brain team. Her research interests are in statistical modeling and learning, with a focus on generative models and representation learning. She received her Ph.D. degree in statistics from the University of California, Los Angeles (UCLA) in 2021 advised by Song-Chun Zhu and Ying Nian Wu. Prior to that, she received her bachelor&#8217;s degree from Peking University. Her recent research themes include scalable training algorithms of deep generative models, variational inference, and representational models with implications in neuroscience.</div></div><div class="clear"></div>
  </td>
  </tr>

  <tr>
  <td class="aiml-none"><div class="aiml-date"><b>February 21</b></div></td>
  <td class="aiml-none"><div class="aiml-name"><b>No Seminar (Presidents&#8217; Day)</b></div>
  </td>
  </tr>

  <tr>
  <td valign="top"><div class="aiml-date"><b>February 28</b><br><b>DBH 4011</b> &amp;<br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="https://sunipa.github.io/"><img src="http://cml.ics.uci.edu/wp-content/uploads/sunipaDev.jpg" width="150px"><br><b>Sunipa Dev</b></a><br>Research Scientist<br>Ethical AI Team, Google AI<br><br>YouTube Stream: <a href="https://youtu.be/V93uXTBnpFw">https://youtu.be/V93uXTBnpFw</a></div><br>  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Towards Inclusive and Socially Aware Language Technologies</b></a></span></div><div class="togglec clearfix">Large language models are commonly used in different paradigms of natural language processing and machine learning, and are known for their efficiency as well as their overall lack of interpretability. Their data driven approach for emulating human language often results in human biases being encoded and even amplified, potentially leading to cyclic propagation of representational and allocational harm. We discuss in this talk some aspects of detecting, evaluating, and mitigating biases and associated harms in a holistic, inclusive, and culturally-aware manner. In particular, we discuss the disparate impact on society of common language tools that are not inclusive of all gender identities. 
<br><br>
<b>Bio:</b> Sunipa Dev is a Research Scientist on the Ethical AI team at Google AI. Previously, she was an NSF Computing Innovation Fellow at UCLA, before which she completed her PhD at the University of Utah. Her ongoing research focuses on various facets of fairness and interpretability in NLP, including robust measurements of bias, cross-cultural understanding of concepts in NLP, and inclusive language representations.</div></div><div class="clear"></div>
  </td>
  </tr>  

  <tr>
  <td valign="top"><div class="aiml-date"><b>March 7</b><br>Zoom<br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="https://sites.google.com/site/sundararajanmukund/"><img src="http://cml.ics.uci.edu/wp-content/uploads/mukundSundararajan.jpg" width="150px"><br><b>Mukund Sundararajan</b></a><br>Principal Research Scientist<br>Google<br><br>YouTube Stream unavailable, please join via Zoom</div><br>  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Analyzing deep neural networks using attribution</b></a></span></div><div class="togglec clearfix">Predicting cancer from XRays seemed great<br>
Until we discovered the true reason.<br>
The model, in its glory, did fixate<br>
On radiologist markings – treason!<br><br>
We found the issue with attribution:<br>
By blaming pixels for the prediction (<a href="https://arxiv.org/abs/1703.01365">1</a>,<a href="https://arxiv.org/abs/1805.12233">2</a>,<a href="https://arxiv.org/abs/1902.05622">3</a>,<a href="https://arxiv.org/abs/1908.08474">4</a>,<a href="https://arxiv.org/abs/2004.03383">5</a>,<a href="https://arxiv.org/abs/1805.05492">6</a>).<br>
A complement&#8217;ry way to attribute,<br>
is to pay training data, a tribute (<a href="https://arxiv.org/abs/2002.08484">1</a>).<br><br>
If you are int&#8217;rested in <a href="https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus">FTC</a>,<br>
counterfactual theory, <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">SGD</a><br>
Or Shapley values and fine kernel tricks,<br>
Please come attend, unless you have conflicts<br><br>
Should you build deep models down the road,<br>
Use attributions. Takes ten lines of code!  
<br><br>
<b>Bio:</b><br>
There once was an <a href="https://acronyms.thefreedictionary.com/Research+scientist">RS</a> called <a href="https://sites.google.com/site/sundararajanmukund/">MS</a>,<br>
The <a href="https://en.wikipedia.org/wiki/Machine_learning">models</a> he studies are a mess,<br>
A director at Google.<br>
Accurate and frugal,<br>
Explanations are what he likes best.</div></div><div class="clear"></div>
  </td>
  </tr>  

  <tr>
  <td class="aiml-none"><div class="aiml-date"><b>March 14</b></div></td>
  <td class="aiml-none"><div class="aiml-name"><b>No Seminar (Finals Week)</b></div>
  </td>
  </tr> 

</tbody></table>



<p></p>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2022/01/winter-2022/" title="3:22 pm" rel="bookmark"><time class="entry-date genericon" datetime="2022-01-05T15:22:53-08:00">January 5, 2022</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
				
<article id="post-1215" class="post-1215 post type-post status-publish format-standard hentry category-aiml">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2021/04/spring-2021/" title="Permalink to Spring 2021" rel="bookmark">Spring 2021</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		
<h2><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream for all Spring 2021 CML Seminars</a></h2>

<table class="wp-block-table aligncenter" cellpadding="5" border="1">
   <colgroup><col width="100"><col>
</colgroup><tbody>

  <tr>
  <td class="aiml-none"><div class="aiml-date"><b>March 29</b></div></td>
  <td class="aiml-none"><div class="aiml-name"><b>No Seminar</b></div>
  </td>
  </tr>

  <tr>
  <td class="aiml-none"><div class="aiml-date"><b>April 5th</b></div></td>
  <td class="aiml-none"><div class="aiml-name"><b>No Seminar</b></div>
  </td>
  </tr>

  <tr>
  <td valign="top"><div class="aiml-date"><b>April 12th</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="https://sanmi.cs.illinois.edu/"><img src="https://sanmi.cs.illinois.edu/images/Koyejo-Profile.jpg" width="150px"><br><b>Sanmi Koyejo</b></a><br>Assistant Professor<br>Department of Computer Science <br> University of Illinois at Urbana-Champaign<br><br>YouTube Stream: <a href="https://youtu.be/Ehqsp8vRLis">https://youtu.be/Ehqsp8vRLis</a> </div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>The Measurement and Mismeasurement of Trustworthy ML</b></a></span></div><div class="togglec clearfix">Across healthcare, science, and engineering, we increasingly employ machine learning (ML) to automate decision-making that, in turn, affects our lives in profound ways. However, ML can fail, with significant and long-lasting consequences. Reliably measuring such failures is the first step towards building robust and trustworthy learning machines. Consider algorithmic fairness, where widely-deployed fairness metrics can exacerbate group disparities and result in discriminatory outcomes. Moreover, existing metrics are often incompatible. Hence, selecting fairness metrics is an open problem. Measurement is also crucial for robustness, particularly in federated learning with error-prone devices. Here, once again, models constructed using well-accepted robustness metrics can fail. Across ML applications, the dire consequences of mismeasurement are a recurring theme. This talk will outline emerging strategies for addressing the measurement gap in ML and how this impacts trustworthiness.
<br><br>
<b>Bio:</b> Sanmi (Oluwasanmi) Koyejo is an Assistant Professor in the Department of Computer Science at the University of Illinois at Urbana-Champaign. Koyejo&#8217;s research interests are in developing the principles and practice of trustworthy machine learning. Additionally, Koyejo focuses on applications to neuroscience and healthcare. Koyejo completed his Ph.D. in Electrical Engineering at the University of Texas at Austin, advised by Joydeep Ghosh, and completed postdoctoral research at Stanford University. His postdoctoral research was primarily with Russell A. Poldrack and Pradeep Ravikumar. Koyejo has been the recipient of several awards, including a best paper award from the conference on uncertainty in artificial intelligence (UAI), a Sloan Fellowship, a Kavli Fellowship, an IJCAI early career spotlight, and a trainee award from the Organization for Human Brain Mapping (OHBM). Koyejo serves on the board of the Black in AI organization.</div></div><div class="clear"></div>
  </td>
  </tr>
 
  <tr>
  <td valign="top"><div class="aiml-date"><b>April 19th</b><br>Sponsored by the <b><a href="https://create.ics.uci.edu/">Steckler Center for Responsible, Ethical, and Accessible Technology (CREATE)</a></b><br>4 pm<br><i>(Note change in time)</i></div></td>
  <td valign="top"><div class="aiml-name"><a href="https://www.katecrawford.net/about.html"><img src="https://www.katecrawford.net/img/katephoto_cath_muscat_scaled.png" width="150px"><br><b>Kate Crawford</b></a><br>Senior Principal Researcher, Microsoft Research, New York<br> Distinguished Visiting Fellow at the University of Melbourne<br><br>  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Towards an Atlas of AI: A Conversation with Kate Crawford (with Paul Dourish and Geof Bowker, Department of Informatics, UC Irvine)</b></a></span></div><div class="togglec clearfix">Where do the motivating ideas behind Artificial Intelligence come from and what do they imply? What claims to universality or particularity are made by AI systems? How do the movements of ideas, data, and materials shape the present and likely futures of AI development? Join us for a conversation with social scientist and AI scholar Kate Crawford about the intellectual history and geopolitical contexts of contemporary AI research and practice.
<br><br>
<b>Bio:</b> Kate Crawford is a leading scholar of the social and political implications of artificial intelligence. Over her 20-year career, her work has focused on understanding large-scale data systems, machine learning and AI in the wider contexts of history, politics, labor, and the environment. She is a Research Professor of Communication and STS at USC Annenberg, a Senior Principal Researcher at MSR-NYC, and the inaugural Visiting Chair for AI and Justice at the École Normale Supérieure in Paris, In 2021, she will be the Miegunyah Distinguished Visiting Fellow at the University of Melbourne, and has been appointed an Honorary Professor at the University of Sydney. She previously co-founded the AI Now Institute at New York University. Kate has advised policy makers in the United Nations, the Federal Trade Commission, the European Parliament, and the White House.
Her academic research has been published in journals such as Nature, New Media &#038; Society, Science, Technology &#038; Human Values and Information, Communication &#038; Society. Beyond academic journals, Kate has also written for The New York Times, The Atlantic, Harpers’ Magazine, among others.</div></div><div class="clear"></div>
  </td>
  </tr>

  <tr>
  <td valign="top"><div class="aiml-date"><b>April 26th</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="https://yiboyang.com/about/"><img src="https://yiboyang.com/images/profile_20pc.jpg" width="150px"><br><b>Yibo Yang</b></a><br>PhD Student<br>Department of Computer Science<br>University of California, Irvine<br><br>YouTube Stream: <a href="https://youtu.be/1lXKUhBTHWc">https://youtu.be/1lXKUhBTHWc</a> </div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Exploring the limits of lossy data compression with deep learning</a></b></span></div><div class="togglec clearfix">Probabilistic machine learning, particularly deep learning, is reshaping the field of data compression. Recent work has established a close connection between lossy data compression and latent variable models such as variational autoencoders (VAEs), and VAEs are now the building blocks of many learning-based lossy compression algorithms that are trained on massive amounts of unlabeled data.  In this talk, I give a brief overview of learned data compression, including the current paradigm of end-to-end lossy compression with VAEs, and present my research that addresses some of its limitations and explores other possibilities of learned data compression.  First, I present algorithmic improvements inspired by variational inference that push the performance limits of VAE-based lossy compression, resulting in a new state-of-the-art performance on image compression. Then, I introduce a new algorithm that compresses the variational posteriors of pre-trained latent variable models, and allows for variable-bitrate lossy compression with a vanilla VAE. Lastly, I discuss ongoing work that explores fundamental bounds on the theoretical performance of lossy compression algorithms, using the tools of stochastic approximation and deep learning.
<br><br>
<b>Bio:</b> Yibo Yang is a PhD student advised by Stephan Mandt in the Computer Science department at UC Irvine. His research interests include probability theory, information theory, and their applications in statistical machine learning.</div></div><div class="clear"></div>
  </td>
  </tr>
 

  <tr>
  <td valign="top"><div class="aiml-date"><b>May 3rd</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="https://webdocs.cs.ualberta.ca/~santanad/"><img src="https://webdocs.cs.ualberta.ca/~santanad/img/levi_amii.jpg" width="150px"><br><b>Levi Lelis</b></a><br>Assistant Professor<br>Department of Computer Science<br>University of Alberta<br><br>YouTube Stream: <a href="https://youtu.be/76NFMs9pHEE">https://youtu.be/76NFMs9pHEE</a> </div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Policy and Heuristic-Guided Tree Search Algorithms</a></b></span></div><div class="togglec clearfix">In this talk I will describe two tree search algorithms that use a policy to guide the search. I will start with Levin tree search (LTS), a best-first search algorithm that has guarantees on the number of nodes it needs to expand to solve state-space search problems. These guarantees are based on the quality of the policy it employs. I will then describe Policy-Guided Heuristic Search (PHS), another best-first search algorithm that uses both a policy and a heuristic function to guide the search. PHS also has guarantees on the number of nodes it expands, which are based on the quality of the policy and of the heuristic function employed. I will then present empirical results showing that LTS and PHS compare favorably with A*, Weighted A*, Greedy Best-First Search, and PUCT on a set of single-agent shortest-path problems.
<br><br>
<b>Bio:</b>  
Levi Lelis is an Assistant Professor at the University of Alberta, Canada, and a Professor on leave from Universidade Federal de Viçosa, Brazil. Levi is interested in heuristic search, machine learning, and program synthesis.</div></div><div class="clear"></div>
  </td>
  </tr>
 

<tr>
  <td valign="top"><div class="aiml-date"><b>May 10th</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="https://dmelis.github.io/"><img src="https://dmelis.github.io/assets/profile-pics/Profile_pic_Sq.png" width="150px"><br><b>David Alvarez-Melis</b></a><br>Postdoctoral Researcher<br>Microsoft Research New England<br><br>YouTube Stream: <a href=" https://youtu.be/52bQ_XUY2DQ"> https://youtu.be/52bQ_XUY2DQ</a> </div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Ideal made real: machine learning with limited data and interpretable outputs</a></b></span></div><div class="togglec clearfix">Abstract: Success stories in machine learning seem to be ubiquitous, but they tend to be concentrated on ‘ideal’ scenarios where clean labeled data are abundant, evaluation metrics are unambiguous, and operational constraints are rare — if at all existent. But machine learning in practice is rarely so &#8216;pristine&#8217;;  clean data is often scarce, resources are limited, and constraints (e.g., privacy, transparency) abound in most real-life applications. In this talk we will explore how to reconcile these paradigms along two main axes: (i) learning with scarce or heterogeneous data, and (ii) making complex models, such as neural networks, interpretable. 

First, I will present various approaches that I have developed for &#8216;amplifying&#8217; (e.g, merging, transforming, interpolating) datasets based on the theory of Optimal Transport. Through applications in machine translation, transfer learning, and dataset shaping, I will show that besides enjoying sound theoretical footing, these approaches yield efficient and high-performing algorithms. In the second part of the talk, I will present some of my work on designing methods to extract &#8216;explanations&#8217; from complex models and on imposing on them some basic formal notions that I argue any interpretability method should satisfy, but which most lack. Finally, I will present a novel framework for interpretable machine learning that takes inspiration from the study of (human) explanation in the social sciences, and whose evaluation through user studies yields insights about the promise (and limitations) of interpretable AI tools. 
<br><br>
<b>Bio:</b>  
David Alvarez-Melis is a postdoctoral researcher in the Machine Learning and Statistics Group at Microsoft Research, New England. He recently obtained a Ph.D. in computer science from MIT advised by Tommi Jaakkola, and holds B.Sc. and M.S. degrees in mathematics from ITAM and Courant Institute (NYU), respectively. He has previously spent time at IBM Research and is a recipient of CONACYT, Hewlett Packard, and AI2 awards.</div></div><div class="clear"></div>
  </td>
  </tr>
 
 <tr>
  <td valign="top"><div class="aiml-date"><b>May 17th</b><br>
<a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="https://faculty.sites.uci.edu/cnclab/people/"><img src="https://faculty.sites.uci.edu/cnclab/files/2020/08/meganpeters_org_x-scaled-e1598314369441-241x300.jpg" width="150px"><br>
<b>Megan Peters</b></a><br>Assistant Professor<br>Department of Cognitive Sciences<br>UC Irvine<br><br>YouTube Stream: <a href=" https://youtu.be/i9Cenn0stxE"> https://youtu.be/i9Cenn0stxE</a> </div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>How do we evaluate our own uncertainty? Uncovering our metacognitive computations</a></b></span></div><div class="togglec clearfix">Abstract: TBA
<br><br>
<b>Bio:</b> In March 2020 I joined the UCI Department of Cognitive Sciences.  I’m also a Cooperating Researcher in the Department of Decoded Neurofeedback at Advanced Telecommunications Research Institute International in Kyoto, Japan.  Prior to that, from 2017 I was on the faculty at UC Riverside in the Department of Bioengineering.  I received my Ph.D. in computational cognitive neuroscience (psychology) from UCLA, and then was a postdoc there as well. My research aims to reveal how the brain represents and uses uncertainty, and performs adaptive computations based on noisy, incomplete information. I specifically focus on how these abilities support metacognitive evaluations of the quality of (mostly perceptual) decisions, and how these processes might relate to phenomenology and conscious awareness. I use neuroimaging, computational modeling, machine learning and neural stimulation techniques to study these topics. </b></div></div><div class="clear"></div>
  </td>
  </tr>
 

  <tr>
  <td valign="top"><div class="aiml-date"><b>May 24th</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign="top"><div class="aiml-name"><a href="https://www.ics.uci.edu/~jingz31/"><img src="https://www.ics.uci.edu/~jingz31/wp-content/uploads/2020/06/JZ-Yale-picture-1536x1024.jpg" width="250px"><br><b>Jing Zhang</b></a><br>Assistant Professor<br>Department of Computer Science<br>University of California, Irvine<br><br>YouTube Stream: <a href="https://youtu.be/HPPq5Xvlr9c"> https://youtu.be/HPPq5Xvlr9c</a> </div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Effective representation learning to dissect the gene regulatory grammar</a></b></span></div><div class="togglec clearfix">The recent advances in sequencing technologies provide unprecedented opportunities to decipher
the multi-scale gene regulatory grammars at diverse cellular states. Here, we will introduce our
computational efforts on cell/gene representation learning to extract biologically meaningful
information from high-dimensional, sparse, and noisy genomic data. First, we proposed a deep
generative model, named SAILER, to learn the low-dimensional latent cell representations from
single-cell epigenetic data for accurate cell state characterization. SAILER adopted the
conventional encoder-decoder framework and imposed additional constraints for biologically
robust cell embeddings invariant to confounding factors. Then at the network level, we
developed TopicNet using latent Dirichlet allocation (LDA) to extract latent gene communities
and quantify regulatory network connectivity changes (network “rewiring”) between diverse cell
states. We applied our TopicNet model on 13 different cancer types and highlighted gene
communities that impact patient prognosis in multiple cancer types.
<br><br>
<b>Bio:</b> Dr. Zhang is an Assistant Professor at UCI. Her research interests are in the areas of
bioinformatics and computational biology. She graduated from USC Electrical Engineering
under the supervision of Dr. Liang Chen and Dr. C.C Jay Kuo. She completed her postdoc
training at Yale University in Dr. Mark Gerstein&#8217;s lab. During her postdoc, she has developed
several computational methods to integrate novel high-throughput sequencing assays to decipher
the gene regulation “grammar”. Her current research focuses on developing computational
methods to predict the impact of genomic variations on genome function and phenotype at a
single-cell resolution.</div></div><div class="clear"></div>
  </td>
  </tr>
 

  <tr>
  <td class="aiml-none"><div class="aiml-date"><b>May 31</b></div></td>
  <td class="aiml-none"><div class="aiml-name"><b>No Seminar (Memorial Day)</b></div>
  </td>
  </tr>

  <tr>
  <td valign="top" class="aiml-none"><div class="aiml-date"><b>June 7th</b></div></td>
  <td valign="top" class="aiml-none"><div class="aiml-name"><b>No Seminar (Finals Week)</b></div>
  </td>
  </tr>


</tbody></table>



<p></p>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2021/04/spring-2021/" title="10:36 am" rel="bookmark"><time class="entry-date genericon" datetime="2021-04-06T10:36:38-07:00">April 6, 2021</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
				
<article id="post-1159" class="post-1159 post type-post status-publish format-standard hentry category-aiml">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2021/01/winter-2021/" title="Permalink to Winter 2021" rel="bookmark">Winter 2021</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		
<h2><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream for all Winter 2021 CML Seminars</a></h2>

<table class="wp-block-table aligncenter"  cellpadding=5 border=1>
   <col width="100"><col>
<tbody>

  <tr>
  <td class='aiml-none'><div class="aiml-date"><b>Jan. 4</b></div></td>
  <td class='aiml-none'><div class="aiml-name"><b>No Seminar</b></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Jan. 11</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="http://florianwenzel.com/"><img src="http://cml.ics.uci.edu/wp-content/uploads/florian-wenzel.jpg" width=200px /><br><b>Florian Wenzel</b></a><br>Postdoctoral Researcher<br>Google Brain Berlin<br><br>YouTube Stream:  <a href="https://youtu.be/9n8_5tjt_Lw">https://youtu.be/9n8_5tjt_Lw</a></div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Towards Reliable Deep Learning</b></a></span></div><div class="togglec clearfix">Deep learning models are bad at detecting their failure. They tend to make over-confident mistakes, especially, under distribution shift. Making deep learning more reliable is important in safety-critical applications including health care, self-driving cars, and recommender systems. We discuss two approaches to reliable deep learning. First, we will focus on Bayesian neural networks that come with many promises to improved uncertainty estimation. However, why are they rarely used in industrial practice? In this talk, we will cast doubt on the current understanding of Bayes posteriors in deep networks. We show that Bayesian neural networks can be improved significantly through the use of a &#8220;cold posterior&#8221; that overcounts evidence and hence sharply deviates from the Bayesian paradigm. We will discuss several hypotheses that could explain cold posteriors. In the second part, we will discuss a classical approach to more robust predictions: ensembles. Deep ensembles combine the predictions of models trained from different initializations. We will show that the diversity of predictions can be improved by considering models with different hyperparameters. Finally, we present an efficient method that leverages hyperparameter diversity within a single model.
<br><br>
<b>Bio:</b> Florian Wenzel is a machine learning researcher who is currently on the job market. His research has focused on probabilistic deep learning, uncertainty estimation, and scalable inference methods. From October 2019 to October 2020 he was a postdoctoral researcher at Google Brain. He received his PhD from Humboldt University in Berlin and worked with Marius Kloft, Stephan Mandt, and Manfred Opper.</div></div><div class="clear"></div>
  </td>
  </tr>

  <tr>
  <td class='aiml-none'><div class="aiml-date"><b>Jan. 18</b></div></td>
  <td class='aiml-none'><div class="aiml-name"><b>No Seminar (Martin Luther King, Jr. Holiday)</b></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Jan. 25</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://yezhouyang.engineering.asu.edu/"><img src="http://cml.ics.uci.edu/wp-content/uploads/yezhou-yang.jpg" width=200px /><br><b>Yezhou Yang</b></a><br>Assistant Professor<br>School of Computing, Informatics, and Decision Systems Engineering<br>Arizona State University<br><br>YouTube Stream:  <a href="https://youtu.be/IcSUBZraB3s">https://youtu.be/IcSUBZraB3s</a></div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Visual Recognition beyond Appearances, and its Robotic Applications</b></a></span></div><div class="togglec clearfix">The goal of Computer Vision, as coined by Marr, is to develop algorithms to answer What are Where at When from visual appearance. The speaker, among others, recognizes the importance of studying underlying entities and relations beyond visual appearance, following an Active Perception paradigm. This talk will present the speaker&#8217;s efforts over the last decade, ranging from 1) reasoning beyond appearance for visual question answering, image understanding and video captioning tasks, through 2) temporal knowledge distillation with incremental knowledge transfer, till 3) their roles in a Robotic visual learning framework via a Robotic Indoor Object Search task. The talk will also feature the Active Perception Group (APG)’s ongoing projects (NSF RI, NRI and CPS, DARPA KAIROS, and Arizona IAM) addressing emerging challenges of the nation in autonomous driving, AI security and healthcare domains, at the ASU School of Computing, Informatics, and Decision Systems Engineering (CIDSE).
<br><br>
<b>Bio:</b> Yezhou Yang is an Assistant Professor at School of Computing, Informatics, and Decision Systems Engineering, Arizona State University.  He is directing the ASU Active Perception Group. His primary interests lie in Cognitive Robotics, Computer Vision, and Robot Vision, especially exploring visual primitives in human action understanding from visual input, grounding them by natural language as well as high-level reasoning over the primitives for intelligent robots. Before joining ASU, Dr. Yang was a Postdoctoral Research Associate at the Computer Vision Lab and the Perception and Robotics Lab, with the University of Maryland Institute for Advanced Computer Studies. He is a recipient of Qualcomm Innovation Fellowship 2011, the NSF CAREER award 2018 and the Amazon AWS Machine Learning Research Award 2019. He receives his Ph.D. from University of Maryland at College Park, and B.E. from Zhejiang University, China.</div></div><div class="clear"></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Feb. 1</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://joelouismarino.github.io/"><img src="http://cml.ics.uci.edu/wp-content/uploads/joe-marino.jpg" width=200px /><br><b>Joe Marino</b></a><br>PhD Student<br>Computation and Neural Systems<br>California Institute of Technology<br><br>YouTube Stream: <a href="https://youtu.be/iVz6uwD7i6A">https://youtu.be/iVz6uwD7i6A</a></div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Connecting Variational Autoencoders Back to the Brain</b></a></span></div><div class="togglec clearfix">Unsupervised machine learning has recently dramatically improved our ability to model and extract structure from data. One such approach is deep latent variable models, which includes variational autoencoders (VAEs) [Kingma &#038; Welling, 2014; Rezende et al., 2014]. These models can be traced back to the Helmholtz machine [Dayan et al., 1995], which, in turn, was inspired by ideas from theoretical neuroscience [Mumford, 1992]. In the intervening years, neuroscientists have further developed these ideas into a popular theory: predictive coding [Rao &#038; Ballard, 1999; Friston, 2005]. Yet, the machine learning community remains largely unaware of these connections. In this talk, I discuss the links between modern deep latent variable models and predictive coding, yielding several striking implications for the correspondences between machine learning and neuroscience. This motivates a more nuanced view in connecting these fields, including the search for backpropagation in the brain.
<br><br>
<b>Bio:</b> Joe Marino is a PhD candidate in the Computation &#038; Neural Systems program at Caltech, advised by Yisong Yue. His work focuses on improving probabilistic models and inference techniques, using neuroscience-inspired ideas, within the areas of generative modeling and reinforcement learning.</div></div><div class="clear"></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Feb. 8</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://scholar.google.com/citations?user=kigtlXEAAAAJ&#038;hl=en"><img src="http://cml.ics.uci.edu/wp-content/uploads/junkyuLee.jpeg" width=200px /><br><b>Junkyu Lee</b></a><br>AI Planning Group<br>IBM Research<br><br>YouTube Stream:  <a href="https://youtu.be/p7X-L1T9ULk">https://youtu.be/p7X-L1T9ULk</a></div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Decomposition Bounds for Influence Diagrams</b></a></span></div><div class="togglec clearfix">Influence diagrams (IDs) extend Bayesian networks with decision variables and utility functions to model the interaction between an agent and a system to capture the preferences.  The standard task in IDs is to compute the maximum expected utility (MEU) over the influence diagram and optimal policies.  However, it is the most challenging task in graphical models. Therefore, computing upper bounds on the MEU is desirable because upper bounds can facilitate anytime-solutions by acting as heuristics to guide search or sampling-based methods. In this talk, I will present bounding schemes for solving IDs. The first approach builds on top of the tree decomposition scheme in probabilistic graphical models and extends variational decomposition bounds in marginal MAP. The second approach is a new tree decomposition method called submodel tree decomposition.  The empirical evaluation results show that presented bounding schemes generate upper bounds that are orders of magnitude tighter than previous methods. Finally, I will conclude the talk with future directions.
<br><br>
<b>Bio:</b> Junkyu Lee received his Ph.D. from the CS department at UC Irvine, where Rina Dechter supervised him. Currently, he is a resident at the IBM Research AI planning group. His research focuses on graphical model inference and heuristic search for sequential decision making under uncertainty. He is also broadly interested in related areas such as planning and reinforcement learning.</div></div><div class="clear"></div>
  </td>
  </tr>

  <tr>
  <td class='aiml-none'><div class="aiml-date"><b>Feb. 15</b></div></td>
  <td class='aiml-none'><div class="aiml-name"><b>No Seminar (Presidents&#8217; Holiday)</b></div>
  </td>
  </tr>

  <tr>
  <td class='aiml-none'><div class="aiml-date"><b>Feb. 22</b></div></td>
  <td class='aiml-none'><div class="aiml-name"><b>No Seminar</b></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>March 1</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://rloganiv.github.io/"><img src="http://cml.ics.uci.edu/wp-content/uploads/robert-logan.jpg" width=200px /><br><b>Robert Logan</b></a><br>PhD Student<br>Department of Computer Science<br>University of California, Irvine<br><br>YouTube Stream:  <a href="https://youtu.be/Mim1pmEn1UU">https://youtu.be/Mim1pmEn1UU</a></div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Fill in the _____: Prompt-Based Solutions for NLP</b></a></span></div><div class="togglec clearfix">Recent progress in natural language processing (NLP) has been predominantly driven by the advent of large neural language models (e.g., GPT-2 and BERT) that are &#8220;pretrained&#8221; using a self-supervised learning objective on billions of tokens of text before being &#8220;finetuned&#8221; (i.e., transferred) to downstream tasks. The exceptional success of these models has motivated many NLP researchers to study what exactly these models are learning during pretraining that causes them to be more successful than their non-self-supervised counterparts. In this talk, we will describe the technique of prompting, an approach that answers this question by reformulating tasks as fill-in-the-blanks questions. We will begin by showing how prompts can be used to measure the amount of factual, linguistic, and task-specific knowledge contained in language models. We will then introduce an approach for automatically constructing prompts based on gradient-guided search that provides a scalable alternative to manually writing prompts by hand. Lastly, we will cover our ongoing work investigating whether prompting can be used as a replacement for finetuning of language models, describing some early results that demonstrate that prompting can indeed be more effective in few-shot learning scenarios while being substantially more parameter efficient.
<br><br>
<b>Bio:</b> Robert L. Logan IV is a 4th year PhD Candidate at UC Irvine, co-advised by Sameer Singh and Padhraic Smyth. His research focuses on leveraging external knowledge sources to measure and improve NLP models&#8217; ability to reason with factual and common sense knowledge. He was selected as a Noyce Fellow and has been awarded the 2020 Rose Hills Foundation Scholarship. Robert received his B.A. in mathematics at the University of California, Santa Cruz, and has held research positions at Google and Diffbot.</div></div><div class="clear"></div>
  </td>
  </tr>

  <tr>
  <td class='aiml-none'><div class="aiml-date"><b>March 8</b></div></td>
  <td class='aiml-none'><div class="aiml-name"><b>No Seminar</b></div>
  </td>
  </tr>

  <tr>
  <td valign=top  class='aiml-none'><div class="aiml-date"><b>March 15</b></div></td>
  <td valign=top  class='aiml-none'><div class="aiml-name"><b>Finals Week</b></div>
  </td>
  </tr>


</tbody></table>



<p></p>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2021/01/winter-2021/" title="10:47 am" rel="bookmark"><time class="entry-date genericon" datetime="2021-01-06T10:47:17-08:00">January 6, 2021</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
				
<article id="post-1052" class="post-1052 post type-post status-publish format-standard hentry category-aiml">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2020/10/fall-2020/" title="Permalink to Fall 2020" rel="bookmark">Fall 2020</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		
<h2><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream for all Fall 2020 CML Seminars</a></h2>

<table class="wp-block-table aligncenter"  cellpadding=5 border=1>
   <col width="100"><col>
<tbody>

  <tr>
  <td class='aiml-none'><div class="aiml-date"><b>Oct 5</b></div></td>
  <td class='aiml-none'><div class="aiml-name"><b>No Seminar</b></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Oct 12</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://cse.sc.edu/~foresta/"><img src="http://cml.ics.uci.edu/wp-content/uploads/forest-agostinelli.jpg" width=200px /><br><b>Forest Agostinelli</b></a><br>Assistant Professor<br>Computer Science and Engineering<br>University of South Carolina<br><br>YouTube Stream:  <a href="https://youtu.be/shwYW9yEAIQ">https://youtu.be/shwYW9yEAIQ</a></div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>From Combination Puzzles to the Natural Sciences</b></a></span></div><div class="togglec clearfix">Combination puzzles, such as the Rubik’s cube, pose unique challenges for artificial intelligence. Furthermore, solutions to such puzzles are directly linked to problems in the natural sciences. In this talk, I will present DeepCubeA, a deep reinforcement learning and search algorithm that can solve the Rubik’s cube, and six other puzzles, without domain specific knowledge. Next, I will discuss how solving combination puzzles opens up new possibilities for solving problems in the natural sciences. Finally, I will show how problems we encounter in the natural sciences motivate future research directions in areas such as theorem proving and education. A demonstration of our work can be seen at <a href="http://deepcube.igb.uci.edu/">http://deepcube.igb.uci.edu/</a>.
<br><br>
<b>Bio:</b> Forest Agostinelli is an assistant professor at the University of South Carolina. He received his B.S. from the Ohio State University, his M.S. from the University of Michigan, and his Ph.D. from UC, Irvine under Professor Pierre Baldi. His research interests include deep learning, reinforcement learning, search, bioinformatics, neuroscience, and chemistry.</div></div><div class="clear"></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Oct 19</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="http://www.stephanmandt.com/"><img src="http://cml.ics.uci.edu/wp-content/uploads/stephan-mandt.jpeg" width=200px /><br><b>Stephan Mandt</b></a><br>Assistant Professor<br>Dept. of Computer Science<br>University of California, Irvine<br><br>YouTube Stream:  <a href="https://youtu.be/Z8juQKrCkmk">https://youtu.be/Z8juQKrCkmk</a></div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Compressing Variational Bayes</b></a></span></div><div class="togglec clearfix">Neural image compression algorithms have recently outperformed their classical counterparts in rate-distortion performance and show great potential to also revolutionize video coding. In this talk, I will show how innovations from Bayesian machine learning and generative modeling can lead to dramatic performance improvements in compression. In particular, I will explain how sequential variational autoencoders can be converted into video codecs, how deep latent variable models can be compressed in post-processing with variable bitrates, and how iterative amortized inference can be used to achieve the world record in image compression performance. 
<br><br>
<b>Bio:</b> Stephan Mandt is an Assistant Professor of Computer Science at the University of California, Irvine. From 2016 until 2018, he was a Senior Researcher and Head of the statistical machine learning group at Disney Research, first in Pittsburgh and later in Los Angeles. He held previous postdoctoral positions at Columbia University and Princeton University. Stephan holds a Ph.D. in Theoretical Physics from the University of Cologne. He is a Fellow of the German National Merit Foundation, a Kavli Fellow of the U.S. National Academy of Sciences, and was a visiting researcher at Google Brain. Stephan regularly serves as an Area Chair for NeurIPS, ICML, AAAI, and ICLR, and is a member of the Editorial Board of JMLR. His research is currently supported by NSF, DARPA, Intel, and Qualcomm.</div></div><div class="clear"></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Oct 26</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://hpi.de/en/research/research-groups/digital-health-machine-learning.html"><img src="http://cml.ics.uci.edu/wp-content/uploads/ChristophLippert.jpeg" width=200px/><br><b>Christoph Lippert</b></a><br>Professor<br>Hasso Plattner Institute<br>University of Potsdam<br><br>YouTube Stream:  <a href="https://youtu.be/zElgAKf4AhE">https://youtu.be/zElgAKf4AhE</a></div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>GWAS of Images using Deep Transfer Learning</b></a></span></div><div class="togglec clearfix">At the Chair of Digital Health &#038; Machine Learning, we are developing methods for the statistical analysis of large biomedical data. In particular imaging provides a powerful means for measuring phenotypic information at scale. While images are abundantly available in large repositories such as the UK Biobank, the analysis of imaging data poses new challenges for statistical methods development. In this talk, I will give an overview over some of our current efforts in using deep representation learning as a non-parametric way to model imaging phenotypes and for associating images to the genome.
<br><br>
<b>References:</b>
<br>
Kirchler, M., Khorasani, S., Kloft, M., &#038; Lippert, C. (2020, June). Two-sample testing using deep learning. In International Conference on Artificial Intelligence and Statistics (pp. 1387-1398). PMLR.
<br>
Kirchler, M., Konigroski, S., Schurmann, C., Norden, M., Meltendorf, C., Kloft, M., Lippert, C. transferGWAS: GWAS of images using deep transfer learning. Manuscript in preparation.
<br>
<b>Bio:</b> Lippert studied bioinformatics from 2001–2008 in Munich and went on to earn his doctorate at the Max Planck Institutes for Intelligent Systems and for Developmental Biology in Tübingen in machine learning bioinformatics, with an emphasis on methods for genome-associated studies. In 2012, he accepted a Researcher position at Microsoft Research in Los Angeles and subsequently carried out work at Human Longevity, Inc. in Mountain View. In 2017, Lippert returned to Germany to head the research group &#8220;Statistical Genomics&#8221; at the Max Delbrück Center for Molecular Medicine in Berlin. In 2018, Lippert has been appointed Full Professor of &#8220;Digital Health &#038; Machine Learning&#8221; in the joint Digital Engineering Faculty of the Hasso Plattner Institute and the University of Potsdam.</div></div><div class="clear"></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Nov 2</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://www.ics.uci.edu/~scottcb/"><img src="http://cml.ics.uci.edu/wp-content/uploads/cory-scott.jpg" width=200px/><br><b>Cory Scott</b></a><br>PhD Student<br>Dept. of Computer Science<br>University of California, Irvine<br><br>YouTube Stream:  <a href="https://youtu.be/CpGfCA92rMw">https://youtu.be/CpGfCA92rMw</a></div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Efficient Learning of Cytoskeletal Dynamics with Multiscale Machine Learning and Optimized Projection Operators</b></a></span></div><div class="togglec clearfix">Microtubules are a primary constituent of the dynamic cytoskeleton in living cells, involved in many cellular processes whose study would benefit from scalable dynamic computational models. We define a novel machine learning model which aggregates information across multiple spatial scales to predict energy potentials measured from a simulation of a section of microtubule. Using projection operators which optimize an objective function related to the diffusion kernel of a graph, we sum information from local neighborhoods. This process is repeated recursively until the coarsest scale, and all scales are separately used as the input to a Graph Convolutional Network, forming our novel architecture: the Graph Prolongation Convolutional Network (GPCN). The GPCN outputs a prediction for each spatial scale, and these are combined using the inverse of the optimized projections. This fine-to-coarse mapping, and its inverse, create a model which is able to learn to predict energetic potentials more efficiently than other GCN ensembles which do not leverage multiscale information. We also compare the effect of training this ensemble in a coarse-to-fine fashion, and find that schedules adapted from the Algebraic Multigrid (AMG) literature further increase this efficiency. Since forces are derivatives of energies, we discuss the implications of this type of model for machine learning of multiscale molecular dynamics.
<br><br>
<b>Reference:</b> C.B. Scott and Eric Mjolsness. “Graph Prolongation Convolutional Networks: Explicitly Multiscale Machine Learning on Graphs with Applications to Modeling of Cytoskeleton”. In: Machine Learning: Science and Technology (2020). DOI: <a href="
https://iopscience.iop.org/article/10.1088/2632-2153/abb6d2">
https://iopscience.iop.org/article/10.1088/2632-2153/abb6d2</a></div></div><div class="clear"></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Nov 9</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://scholar.google.com/citations?user=40QzNXMAAAAJ&#038;hl=en"><img src="http://cml.ics.uci.edu/wp-content/uploads/lukas-ruff.jpeg" width=200px/><br><b>Lukas Ruff</b></a><br>PhD Student<br>Electrical Engineering and Computer Science<br>TU Berlin<br><br>YouTube Stream:  <a href="https://youtu.be/Uncc5y7g8Is">https://youtu.be/Uncc5y7g8Is</a></div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Recent Advances in Anomaly Detection</b></a></span></div><div class="togglec clearfix">Anomaly detection is the problem of identifying unusual observations in data. This problem is usually unsupervised and occurs in numerous applications such as industrial fault and damage detection, fraud detection in finance and insurance, intrusion detection in cybersecurity, scientific discovery, or medical diagnosis and disease detection. Many of these applications involve complex data such as images, text, graphs, or biological sequences, that is continually growing in size. This has sparked a great interest in developing deep learning approaches to anomaly detection.
<br>
In this talk, my aim is to provide a systematic and unifying overview of deep anomaly detection methods. We will discuss methods based on reconstruction, generative modeling, and one-class classification, where we identify common underlying principles and draw connections between traditional &#8216;shallow&#8217; and novel deep methods. Furthermore, we will cover recent developments that include weakly and self-supervised approaches as well as techniques for explaining models that enable to reveal &#8216;Clever Hans&#8217; detectors. Finally, I will conclude the talk by highlighting some open challenges and potential paths for future research.
<br><br>
<b>Bio:</b> Lukas Ruff is a third year PhD student in the Machine Learning Group headed by Klaus-Robert Müller at TU Berlin. His research covers robust and trustworthy machine learning, with a specific focus on deep anomaly detection. Lukas received a B.Sc. degree in Mathematical Finance from the University of Konstanz in 2015 and a joint M.Sc. degree in Statistics from HU, TU and FU Berlin in 2017.</div></div><div class="clear"></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Nov 16</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://web.eecs.umich.edu/~karem/"><img src="http://cml.ics.uci.edu/wp-content/uploads/karem-sakallah.jpg" width=200px/><br><b>Karem Sakallah</b></a><br>Professor<br>Electrical Engineering and Computer Science<br>University of Michigan<br><br>YouTube Stream:  <a href="https://youtu.be/5A5dTRo50EQ">https://youtu.be/5A5dTRo50EQ</a></div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Accidental Research: Scalable Algorithms for Boolean Satisfiability and Graph Automorphism</b></a></span></div><div class="togglec clearfix">Accidental research is when you&#8217;re an expert in some domain and seek to solve problem A in that domain. You soon discover that to solve A you need to also solve B which, however, comes from a domain in which you have little, or even no, expertise. You, thus, explore existing solutions to B but are disappointed to find that they just aren&#8217;t up to the task of solving A. Your options at this point are a) to abandon this futile project, or b) to try and find a solution to B that will help you solve A. While this might seem like a fool&#8217;s errand, you have the advantage over B experts of being unencumbered by their experience. You are a novice who does not, yet, appreciate the complexity of B, but are able to explore it from a fresh perspective. You also bring along expertise from your own domain to connect what you know with what you hope to learn. If you&#8217;re lucky, you may succeed in finding a solution to B that helps you solve A.
<br>
I will relate two cases in which this scenario played out: developing the GRASP conflict-driven clause-learning SAT solver in the context of performing timing analysis of very large scale integrated circuits, and developing the saucy graph automorphism  program to find and break symmetries in large SAT problems. Ironically, in both cases solving problem B (GRASP, saucy) turned out to be much more impactful than solving problem A (timing analysis, breaking symmetries.) Without the trigger of problem A, however, neither GRASP nor saucy would have been conceived.
<br><br>
<b>Bio:</b> Karem A. Sakallah  is a Professor of Electrical Engineering and Computer Science at the University of Michigan.
He received  the  B.E.  degree  in electrical engineering from the American University of Beirut and the M.S. and Ph.D. degrees in electrical and computer engineering from Carnegie Mellon University.  Prior to joining the University of Michigan, he headed the Analysis and Simulation Advanced Development Team at Digital Equipment Corporation. Besides his academic duties, he has served in a variety of professional roles including the establishment of a computing research institute in Qatar for which he took a leave to serve a term of three years as the Chief Scientist.  His current research is focused on automating the formal verification of hardware, software, and distributed protocols.  He is a fellow of the IEEE and the ACM and a co-recipient  of  the  prestigious  Computer-Aided  Verification  Award  for  “Fundamental  contributions  to  the development of high-performance Boolean satisfiability solvers.”</div></div><div class="clear"></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Nov 23</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://panageas.github.io/"><img src="http://cml.ics.uci.edu/wp-content/uploads/IoannisPanageas.jpeg" width=200px/><br><b>Ioannis Panageas</b></a><br>Assistant Professor<br>Dept. of Computer Science<br>University of California, Irvine<br><br>YouTube Stream:  <a href="https://youtu.be/4cepfWDiL3A">https://youtu.be/4cepfWDiL3A</a></div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>First-order Methods Almost Always Avoid Saddle Points</b></a></span></div><div class="togglec clearfix">In this talk we will give an overview of some results on the limiting behavior of first-order methods. In particular we will show that typical instantiations of first-order methods like gradient descent, coordinate descent, etc. avoid saddle points for almost all initializations. Moreover, we will provide applications of these results on Non-negative Matrix Factorization. The takeaway message is that such algorithms can be studied from a dynamical systems perspective in which appropriate instantiations of the Stable Manifold Theorem allow for a global stability analysis.
<br><br>
<b>Bio:</b> Ioannis is an Assistant Professor of Computer Science at UCI. He is interested in the theory of computation, machine learning and its interface with non-convex optimization, dynamical systems, probability and statistics. Before joining UCI, he was an Assistant Professor at Singapore University of Technology and Design. Prior to that he was a MIT postdoctoral fellow working with Constantinos Daskalakis. He received his PhD in Algorithms, Combinatorics and Optimization from Georgia Tech in 2016, a Diploma in EECS from National Technical University of Athens, and a M.Sc. in Mathematics from Georgia Tech. He is the recipient of the 2019 NRF fellowship for AI.</div></div><div class="clear"></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Nov 30</b><br><a href="https://cml.ics.uci.edu/aiml/cml-seminar-live-stream/">Live Stream</a><br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://deqings.github.io/"><img src="http://cml.ics.uci.edu/wp-content/uploads/DeqingSun.jpeg" width=200px/><br><b>Deqing Sun</b></a><br>Senior Research Scientist<br>Google<br><br>YouTube Stream: <a href="https://youtu.be/N3y_K1ewkL0">https://youtu.be/N3y_K1ewkL0</a></div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Learning Optical Flow: From MRFs to CNNs</b></a></span></div><div class="togglec clearfix">Optical flow provides important motion information about the dynamic world and is of fundamental importance to many tasks. Like other visual inference problems, it is critical to choose the representation to encode both the forward formation process and the prior knowledge of optical flow. In this talk, I will present my work on two different optical flow representations in the past decade. First, I will describe learning Markov random field (MRF) models and defining non-local conditional random field (CRF) models to recover motion boundaries. Second, I will talk about combining domain knowledge of optical flow with convolutional neural networks (CNNs) to develop a compact and effective model and some recent developments. 
<br><br>
<b>Bio:</b> Deqing Sun is a senior research scientist at Google working on computer vision and machine learning. He received a Ph.D. degree in Computer Science from Brown University. He is a recipient of the PAMI Young Researcher award in 2020, the Longuet-Higgins prize at CVPR 2020, the best paper honorable mention award at CVPR 2018, and the first prize in the robust optical flow competition at CVPR 2018 and ECCV 2020. He served as an area chair for CVPR/ECCV/BMVC, and co-organized several workshops/tutorials at CVPR, ECCV, and SIGGRAPH.</div></div><div class="clear"></div>
  </td>
  </tr>

  <tr>
  <td class='aiml-none'><div class="aiml-date"><b>Dec 7</b></div></td>
  <td class='aiml-none'><div class="aiml-name"><b>No Seminar (<a href="https://neurips.cc/">NeurIPS Conference</a>)</b></div>
  </td>
  </tr>


  <tr>
  <td valign=top  class='aiml-none'><div class="aiml-date"><b>Dec 14</b></div></td>
  <td valign=top  class='aiml-none'><div class="aiml-name"><b>Finals week</b></div>
  </td>
  </tr>


</tbody></table>



<p></p>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2020/10/fall-2020/" title="6:22 pm" rel="bookmark"><time class="entry-date genericon" datetime="2020-10-04T18:22:47-07:00">October 4, 2020</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
				
<article id="post-989" class="post-989 post type-post status-publish format-standard hentry category-aiml">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2020/01/winter-2020/" title="Permalink to Winter 2020" rel="bookmark">Winter 2020</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		
<h2>Spring 2020 Seminars Delayed</h2>
<p>Following <a href="https://chancellor.uci.edu/engagement/campus-communications/2020/200310-covid-19-academic-operational-activity.php">UCI guidance</a> to limit social interactions during the COVID-19 outbreak, our CML seminar series is cancelled for the start of spring quarter.  We hope to rejoin you later this year.</p>

<hr>

<table class="wp-block-table aligncenter"  cellpadding=5 border=1>
   <col width="100"><col>
<tbody>

  <tr>
  <td class='aiml-none'><div class="aiml-date"><b>Jan. 6</b></div></td>
  <td class='aiml-none'><div class="aiml-name"><b>No Seminar</b></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Jan. 13</b><br>4011<br>Bren Hall<br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><img src="http://cml.ics.uci.edu/wp-content/uploads/michaelCampbell.jpg" width=200px /><br><b>Michael Campbell</b></a><br>Eureka (SAP)</div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Speculative and hedging interaction model in oil and U.S. dollar markets: Long-term investor dynamics and phases</b></a></span></div><div class="togglec clearfix">We develop the rational dynamics for the long-term investor among boundedly rational speculators in the Carfì-Musolino speculative and hedging model. Numerical evidence is given that indicates there are various phases determined by the degree of non-rational behavior of speculators. The dynamics are shown to be influenced by speculator “noise”. This model has two types of operators: a real economic subject (Air, a long-term trader) and one or more investment banks (Bank, short-term speculators). It also has two markets: oil spot market and U.S. dollar futures. Bank agents react to Air and equilibrate much more quickly than Air, thus we consider rational, best-local-response dynamics for Air based on averaged values of equilibrated Bank variables. The averaged Bank variables are effectively parameters for Air dynamics that depend on deviations-from-rationality (temperature) and Air investment (external field). At zero field, below a critical temperature, there is a phase transition in the speculator system which creates two equilibriums for bank variables, hence in this regime the parameters for the dynamics of the long-term investor Air can undergo a rapid change, which is exactly what happens in the study of quenched dynamics for physical systems. It is also shown that large changes in strategy by the long-term Air investor are always preceded by diverging spatial volatility of Bank speculators. The phases resemble those for unemployment in the “Mark 0” macroeconomic model.</div></div><div class="clear"></div></div>
  </td>
  </tr>

  <tr>
  <td class='aiml-none'><div class="aiml-date"><b>Jan. 20</b></div></td>
  <td class='aiml-none'><div class="aiml-name"><b>Martin Luther King Junior Day</b></div>
  </td>
  </tr>

  <tr>
  <td class='aiml-none'><div class="aiml-date"><b>Jan. 27</b></div></td>
  <td class='aiml-none'><div class="aiml-name"><b>No Seminar</b></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Feb. 3</b><br>4011<br>Bren Hall<br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://www.cs.uci.edu/ph-d-student-phanwadee-sinthong-and-professor-carey-win-best-student-paper-award/"><img src="http://cml.ics.uci.edu/wp-content/uploads/giftSinthong.jpg" width=200px /><br><b>Phanwadee Sinthong</b></a><br>Computer Science<br>University of California, Irvine</div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>AFrame: Extending DataFrames for Large-Scale Modern Data Analysis</b></a></span></div><div class="togglec clearfix">Analyzing the increasingly large volumes of data that are available today, possibly including the application of custom machine learning models, requires the utilization of distributed frameworks. This can result in serious productivity issues for &#8220;normal&#8221; data scientists. We introduce AFrame, a new scalable data analysis package powered by a Big Data management system that extends the data scientists&#8217; familiar DataFrame operations to efficiently operate on managed data at scale. AFrame is implemented as a layer on top of Apache AsterixDB, transparently scaling out the execution of DataFrame operations and machine learning model invocation through a parallel, shared-nothing big data management system. AFrame allows users to interact with a very large volume of semi-structured data in the same way that Pandas DataFrames work against locally stored tabular data. Our AFrame prototype leverages lazy evaluation. AFrame operations are incrementally translated into AsterixDB SQL++ queries that are executed only when final results are called for. In order to evaluate our proposed approach, we also introduce an extensible micro-benchmark for use in evaluating DataFrame performance in both single-node and distributed settings via a collection of representative analytic operations.
<br><br>
<b>Bio:</b> Phanwadee (Gift) Sinthong is a fourth-year Ph.D. student in the CS Department at UC Irvine, advised by Professor Michael Carey. Her research interests are broadly in data management and distributed computation. Her current project is to deliver a scale-independent data science platform by incorporating database management capabilities with existing data science technologies to help support and enhance big data analysis.</div></div><div class="clear"></div></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Feb. 10</b><br>4011<br>Bren Hall<br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://mingzhang-yin.github.io/"><img src="http://cml.ics.uci.edu/wp-content/uploads/mingzhangYin.jpg" width=200px /><br><b>Mingzhang Yin</b></a><br>Statistics and Data Sciences<br>University of Texas, Austin</div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Semi-Implicit Variational Inference</b></a></span></div><div class="togglec clearfix">Uncertainty estimation is one of the most unique features of biological systems, as we have to sense and act in noisy environments. In this talk, I will introduce semi-implicit variational inference (SIVI) as a new machine-learning framework to achieve accurate uncertainty estimation in general latent variable models. Semi-implicit distribution is introduced to expand the commonly used analytic variational family, by mixing the variational parameters with a highly flexible distribution. To cope with this new distribution family, a novel evidence lower bound is derived to achieve the accurate statistical inference. The theoretical properties of the proposed methods will be introduced from an information-theoretic perspective. With a substantially expanded variational family and a novel optimization algorithm, SIVI is shown to closely match the accuracy of MCMC in inferring the posterior while maintaining the merits of variational methods in a variety of Bayesian inference tasks.
<br><br>
<b>Bio:</b> Mingzhang Yin is a fifth year Ph.D. student in statistics at UT Austin. His research centers around Bayesian methods and machine learning, with a focus on approximate inference and structured data modeling.</div></div><div class="clear"></div></div>
  </td>
  </tr>

  <tr>
  <td class='aiml-none'><div class="aiml-date"><b>Feb. 17</b></div></td>
  <td class='aiml-none'><div class="aiml-name"><b>Presidents&#8217; Day</b></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Feb. 24</b><br>4011<br>Bren Hall<br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://jaan.io/"><img src="http://cml.ics.uci.edu/wp-content/uploads/JaanAltosaar.png" width=200px /><br><b>Jaan Altosaar</b></a><br>Physics Department<br>Princeton University</div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Probabilistic modeling in support of science and decision-making: From statistical physics to recommender systems</b></a></span></div><div class="togglec clearfix">Applied machine learning relies on translating the structure of a problem into a computational model. This arises in applications as diverse as statistical physics and food recommendation. The pattern of connectivity in an undirected graphical model or the fact that datapoints in food recommendation are unordered collections of features can inform the structure of a model. First, consider undirected graphical models from statistical physics like the ubiquitous Ising model. Basic research in statistical physics requires accurate and scalable simulations for comparing the behavior of these models to their experimental counterparts. The Ising model consists of binary random variables with local connectivity; interactions between neighboring nodes can lead to long-range correlations. Modeling these correlations is necessary to capture physical phenomena such as phase transitions. To mirror the local structure of these models, we use flow-based convolutional generative models that can capture long-range correlations. Combining flow-based models designed for continuous variables with recent work on hierarchical variational approximations enables the modeling of discrete random variables. Compared to existing variational inference methods, this approach scales to statistical physics models with tens of thousands of correlated random variables and uses fewer parameters. Just as computational choices can be made by considering the structure of an undirected graphical model, model construction itself can be guided by the structure of individual datapoints. Consider a recommendation task where datapoints consist of unordered sets, and the objective is to maximize top-K recall, a common recommendation metric. Simple results show that a classifier with zero worst-case error achieves maximum top-K recall. Further, the unordered structure of the data suggests the use of a permutation-invariant classifier for statistical and computational efficiency. We evaluate this recommendation model on a dataset of 55k users logging 16M meals on a food tracking app, where every meal is an unordered collection of ingredients. On this data, permutation-invariant classifiers outperform probabilistic matrix factorization methods.
<br><br>
<b>Bio:</b> Jaan Altosaar is a PhD Candidate in the Physics department at Princeton University where he is advised by David Blei and Shivaji Sondhi. He is a visiting academic at the Center for Data Science at New York University, where he works with Kyle Cranmer. His research focuses on machine learning methodology such as developing Bayesian deep learning techniques or variational inference methods for statistical physics. Prior to Princeton, Jaan earned his BSc in Mathematics and Physics from McGill University. He has interned at Google Brain and DeepMind, and his work has been supported by fellowships from the Natural Sciences and Engineering Research Council of Canada.</div></div><div class="clear"></div></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Mar. 2</b><br>6011<br>Bren Hall<br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://allenai.org/team/orene/"><img src="http://cml.ics.uci.edu/wp-content/uploads/OrenSquareCrop.png" width=200px /><br><b>Oren Etzioni</b></a><br>CEO, Allen Institute for Artificial Intelligence (AI2)</div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Artificial Intelligence and the Future of Humanity</b></a></span></div><div class="togglec clearfix">Could we wake up one morning to find that AI is poised to take over the world?  Is AI the technology of unfairness and bias? My talk will assess these concerns, and sketch a more optimistic view. We will have ample warning before the emergence of superintelligence, and in the meantime we have the opportunity to create Beneficial AI:<br>(1) AI that mitigates bias rather than amplifying it.<br>(2) AI that saves lives rather than taking them.<br>(3) AI that helps us to solve humanity’s thorniest problems.<br>My talk builds on work at the Allen Institute for AI,  a non-profit research institute based in Seattle.
<br><br>
<b>Bio:</b> Oren Etzioni launched the Allen Institute for AI, and has served as its CEO since 2014. He has been a Professor at the University of Washington&#8217;s Computer Science department since 1991, publishing papers that have garnered over 2,300 highly influential citations on Semantic Scholar. He is also the founder of several startups including Farecast (acquired by Microsoft in 2008).</div></div><div class="clear"></div></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Mar. 9</b><br>4011<br>Bren Hall<br>12 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://panageas.github.io/"><img src="https://panageas.github.io/images/profile.png" width=200px /><br><b>Ioannis Panageas</b></a><br>Singapore University of Technology and Design</div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Depth-width trade-offs for ReLU networks via Sharkovsky's theorem</b></a></span></div><div class="togglec clearfix">Understanding the representational power of Deep Neural Networks (DNNs) and how their structural properties (e.g., depth, width, type of activation unit) affect the functions they can compute, has been an important yet challenging question in deep learning and approximation theory. In a seminal paper, Telgarsky highlighted the benefits of depth by presenting a family of functions (based on simple triangular waves) for which DNNs achieve zero classification error, whereas shallow networks with fewer than exponentially many nodes incur constant error. Even though Telgarsky&#8217;s work reveals the limitations of shallow neural networks, it does not inform us on why these functions are difficult to represent and in fact he states it as a tantalizing open question to characterize those functions that cannot be well-approximated by smaller depths. In this talk, we will point to a new connection between DNNs expressivity and Sharkovsky&#8217;s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks for representing functions based on the presence of generalized notion of fixed points, called periodic points (a fixed point is a point of period 1). Motivated by our observation that the triangle waves used in Telgarsky&#8217;s work contain points of period 3 &#8211; a period that is special in that it implies chaotic behavior based on the celebrated result by Li-Yorke &#8211; we will give general lower bounds for the width needed to represent periodic functions as a function of the depth. Technically, the crux of our approach is based on an eigenvalue analysis of the dynamical system associated with such functions. 
<br><br>
<b>Bio:</b> Ioannis Panageas is an Assistant Professor at Information Systems Department of SUTD since September 2018. Prior to that he was a MIT postdoctoral fellow working with Constantinos Daskalakis. He received his PhD in Algorithms, Combinatorics and Optimization from Georgia Institute of Technology in 2016, a Diploma in EECS from National Technical University of Athens (summa cum laude) and a M.Sc. in Mathematics from Georgia Institute of Technology. His work lies on the intersection of optimization, probability, learning theory, dynamical systems and algorithms. He is the recipient of the 2019 NRF fellowship for AI (analogue of NSF CAREER award).</div></div><div class="clear"></div></div>
  </td>
  </tr>

  <tr>
  <td valign=top  class='aiml-none'><div class="aiml-date"><b>Mar. 16</b></div></td>
  <td valign=top  class='aiml-none'><div class="aiml-name"><b>Finals Week</b></div>
  </td>
  </tr>

  <tr>
  <td class='aiml-none'><div class="aiml-date"><b>Mar. 23</b></div></td>
  <td class='aiml-none'><div class="aiml-name"><b>Spring Break</b></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>TBD</b><br>4011<br>Bren Hall</div></td>
  <td valign=top ><div class="aiml-name"><a href="http://qning2.web.engr.illinois.edu/"><img src="http://cml.ics.uci.edu/wp-content/uploads/QiangNing.jpg" width=200px /><br><b>Qiang Ning</b></a><br>Allen Institute for AI</div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Understanding events in natural language: Learning, common sense, annotation, and what's next</b></a></span></div><div class="togglec clearfix">The era of information explosion has opened up an unprecedented opportunity to study the social, political, financial and medical events described in natural language text. While the past decades have seen significant progress in deep learning and natural language processing (NLP), it is still extremely difficult to analyze textual data at the event-level, e.g., to understand what is going on, what is the cause and impact, and how things will unfold over time.<br>In this talk, I will mainly focus on a key component of event understanding: temporal relations. Understanding temporal relations is challenging due to the lack of explicit timestamps in natural language text, its strong dependence on background knowledge, and the difficulty of collecting high-quality annotations to train models. I will present a series of results addressing these problems from the perspective of structured learning, common sense knowledge acquisition, and data annotation. These efforts culminated in improving the state-of-the-art by approximately 20% in absolute F1. I will also discuss recent results on other aspects of event understanding and the incidental supervision paradigm. I will conclude my talk by describing my vision on future directions towards building next-generation event-based NLP techniques.
<br><br>
<b>Bio:</b> Qiang Ning is a research scientist on the AllenNLP team at the Allen Institute for AI (AI2). Qiang received his Ph.D. in Dec. 2019 from the Department of Electrical and Computer Engineering at the University of Illinois at Urbana-Champaign (UIUC). He obtained his master’s degree in biomedical imaging from the same department in May 2016. Before coming to the United States, Qiang obtained two bachelor’s degrees from Tsinghua University in 2013, in Electronic Engineering and in Economics, respectively. He was an “Excellent Teacher Ranked by Their Students” across the university in 2017 (UIUC), a recipient of the YEE Fellowship in 2015, a finalist for the best paper in IEEE ISBI’15, and also won the National Scholarship at Tsinghua University in 2012.</div></div><div class="clear"></div></div>
  </td>
  </tr>

</tbody></table>



<p></p>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2020/01/winter-2020/" title="3:13 pm" rel="bookmark"><time class="entry-date genericon" datetime="2020-01-10T15:13:05-08:00">January 10, 2020</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
				
<article id="post-865" class="post-865 post type-post status-publish format-standard hentry category-aiml">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2019/09/fall-2019/" title="Permalink to Fall 2019" rel="bookmark">Fall 2019</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		
<table class="wp-block-table aligncenter"  cellpadding=5 border=1>
   <col width="100"><col>
<tbody>

  <tr>
  <td class='aiml-none'><div class="aiml-date"><b>Sep 23</b></div></td>
  <td class='aiml-none'><div class="aiml-name"><b>No Seminar</b></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Sep 30</b><br>4011<br>Bren Hall<br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="http://niadowell.com/"><img src="http://cml.ics.uci.edu/wp-content/uploads/IMG_8437.jpg" width=200px /><br><b>Nia Dowell</b></a><br>Assistant Professor<br>School of Education<br>University of California, Irvine</div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Group Communication Analysis: Applications for Online Learning Environments</b></a></span></div><div class="togglec clearfix">Educational environments have become increasingly reliant on computer-mediated communication, relying on video conferencing, synchronous chats, and asynchronous forums, in both small (5-20 learners) and massive (1000+ learner) learning environments. These platforms, which are designed to support or even supplant traditional instruction, have become common-place across all levels of education, and as a result created big data in education. In order to move forward, the learning sciences field is in need of new automated approaches that offer deeper insights into the dynamics of learner interaction and discourse across online learning platforms. This talk will present results from recent work that uses language and discourse to capture social and cognitive dynamics during collaborative interactions. I will introduce group communication analysis (GCA), a novel approach for detecting emergent learner roles from the participants’ contributions and patterns of interaction. This method makes use of automated computational linguistic analysis of the sequential interactions of participants in online group communication to create distinct interaction profiles. We have applied the GCA to several collaborative learning datasets. Cluster analysis, predictive, and hierarchical linear mixed-effects modeling
were used to assess the validity of the GCA approach, and practical influence of learner roles on student and overall group performance. The results indicate that learners’ patterns in linguistic coordination and cohesion are representative of the roles that individuals play in collaborative discussions. More broadly, GCA provides a framework for researchers to explore the micro intra- and inter-personal patterns associated with the participants’ roles and the sociocognitive processes related to successful collaboration.
<br><br>
<b>Bio:</b> I am an assistant professor in the School of Education at UCI. My primary interests are in cognitive psychology, discourse processing, group interaction, and learning analytics. In general, my research focuses on using language and discourse to uncover the dynamics of socially significant, cognitive, and affective processes. I am currently applying computational techniques to model discourse and social dynamics in a variety of environments including small group computer-mediated collaborative learning environments, collaborative design networks, and massive open online courses (MOOCs). My research has also extended beyond the educational and learning sciences spaces and highlighted the practical applications of computational discourse science in the clinical, political and social sciences areas.</div></div><div class="clear"></div></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Oct 7</b><br>4011<br>Bren Hall<br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://www.ssriva.com"><img src="http://cml.ics.uci.edu/wp-content/uploads/E7478E74-E693-4911-B5CF-61F1A7A87075.png" width=200px /><br><b>Shashank Srivastava</b></a><br>Assistant Professor<br>Computer Science<br>UNC Chapel Hill</div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Conversational Machine Learning</b></a></span></div><div class="togglec clearfix">Humans can efficiently learn and communicate new knowledge about the world through natural language (e.g, the concept of important emails may be described through explanations like ‘late night emails from my boss are usually important’). Can machines be similarly taught new tasks and behavior through natural language interactions with their users? In this talk, we&#8217;ll explore two approaches towards language-based learning for classifications tasks. First, we&#8217;ll consider how language can be leveraged for interactive feature space construction for learning tasks. I&#8217;ll present a method that jointly learns to understand language and learn classification models, by using explanations in conjunction with a small number of labeled examples of the concept. Secondly, we&#8217;ll examine an approach for using language as a substitute for labeled supervision for training machine learning models, which leverages the semantics of quantifier expressions in everyday language (`definitely&#8217;, `sometimes&#8217;, etc.) to enable learning in scenarios with limited or no labeled data. 
<br><br>
<b>Bio:</b> Shashank Srivastava is  an assistant professor in the Computer Science department at the University of North Carolina (UNC) Chapel Hill. Shashank received his PhD from the Machine Learning department at CMU in 2018, and was an AI Resident at Microsoft Research in 2018-19. Shashank&#8217;s research interests lie in conversational AI, interactive machine learning and grounded language understanding. Shashank has an undergraduate degree in Computer Science from IIT Kanpur, and a Master’s degree in Language Technologies from CMU. He received the Yahoo InMind Fellowship for 2016-17; his research has been covered by popular media outlets including GeekWire and New Scientist.</div></div><div class="clear"></div></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Oct 14</b><br>4011<br>Bren Hall<br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="http://www.cs.cmu.edu/~bdhingra/"><img src="http://cml.ics.uci.edu/wp-content/uploads/55B05642-4A63-47D3-91DB-81154AC80E53.jpeg" width=200px /><br><b>Bhuwan Dhingra</b></a><br>PhD Student<br>Language Technologies Institute<br>Carnegie Mellon University</div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Text as a Virtual Knowledge Base</b></a></span></div><div class="togglec clearfix">Structured Knowledge Bases (KBs) are extremely useful for applications such as question answering and dialog, but are difficult to populate and maintain. People prefer expressing information in natural language, and hence text corpora, such as Wikipedia, contain more detailed up-to-date information. This raises the question &#8212; can we directly treat text corpora as knowledge bases for extracting information on demand?
<p>
In this talk I will focus on two problems related to this question. First, I will look at augmenting incomplete KBs with textual knowledge for question answering. I will describe a graph neural network model for processing heterogeneous data from the two sources. Next, I will describe a scalable approach for compositional reasoning over the contents of the text corpus, analogous to following a path of relations in a structured KB to answer multi-hop queries. I will conclude by discussing interesting future research directions in this domain.
<br><br>
<b>Bio:</b> Bhuwan Dhingra is a final year PhD student at Carnegie Mellon University, advised by William Cohen and Ruslan Salakhutdinov. His research uses natural language processing and machine learning to build an interface between AI applications and world knowledge (facts about people, places and things). His work is supported by the Siemens FutureMakers PhD fellowship. Prior to joining CMU, Bhuwan completed his undergraduate studies at IIT Kanpur in 2013, and spent two years at Qualcomm Research in the beautiful city of San Diego.</div></div><div class="clear"></div></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Oct 21</b><br>4011<br>Bren Hall<br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://robamler.github.io"><img src="http://cml.ics.uci.edu/wp-content/uploads/robert-bamler.jpg" width=200px/><br><b>Robert Bamler</b></a><br>Postdoctoral Researcher<br>Dept. of Computer Science<br>University of California, Irvine</div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Revisiting Variational Expectation Maximization</b></a></span></div><div class="togglec clearfix">Bayesian inference is often advertised for applications where posterior uncertainties matter. A less appreciated advantage of Bayesian inference is that it allows for highly scalable model selection (“hyperparameter tuning”) via the Expectation Maximization (EM) algorithm and its approximate variant, variational EM.
In this talk, I will present both an application and an improvement of variational EM. The application is for link prediction in knowledge graphs, where a probabilistic approach and variational EM allowed us to train highly flexible models with more than ten thousand hyperparameters, improving predictive performance. In the second part of the talk, I will propose a new family of objective functions for variational EM. We will see that existing versions of variational inference in the literature can be interpreted as various forms of biased importance sampling of the marginal likelihood. Combining this insight with ideas from perturbation theory in statistical physics will lead us to a tighter bound on the true marginal likelihood and to better predictive performance of Variational Autoencoders.
<br><br>
<b>Bio:</b>
Robert Bamler is a Postdoc at UCI in the group of Prof. Stephan Mandt. His interests are probabilistic embedding models, variational inference, and probabilistic deep learning methods for data compression. Before joining UCI in December of 2018, Rob worked in the statistical machine learning group at Disney Research in Pittsburgh and Los Angeles. He received his PhD in theoretical statistical and quantum physics from University of Cologne, Germany.</div></div><div class="clear"></div></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Oct 28</b><br>4011<br>Bren Hall<br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="http://zhouyu.cs.ucdavis.edu/"><img src="http://cml.ics.uci.edu/wp-content/uploads/D4115C5B-2FC4-4B8C-A707-93940CC236DE.jpeg" width=200px/><br><b>Zhou Yu</b></a><br>Assistant Professor<br>Dept. of Computer Science<br>University of California, Davis</div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Augment intelligence with multimodal information</b></a></span></div><div class="togglec clearfix">Humans interact with other humans or the world through information from various channels including vision, audio, language, haptics, etc.  To simulate intelligence, machines require similar abilities to process and combine information from different channels to acquire better situation awareness, better communication ability, and better decision-making ability. In this talk, we describe three projects. In the first study, we enable a robot to utilize both vision and audio information to achieve better user understanding. Then we use incremental language generation to improve the robot&#8217;s communication with a human. In the second study, we utilize multimodal history tracking to optimize policy planning in task-oriented visual dialogs. In the third project, we tackle the well-known trade-off between dialog response relevance and policy effectiveness in visual dialog generation. We propose a new machine learning procedure that alternates from supervised learning and reinforcement learning to optimum language generation and policy planning jointly in visual dialogs. We will also cover some recent ongoing work on image synthesis through dialogs, and generating social multimodal dialogs with a blend of GIF and words.
<br><br>
<b>Bio:</b>
Zhou Yu is an Assistant Professor at the Computer Science Department at UC Davis. She received her PhD from Carnegie Mellon University in 2017.  Zhou is interested in building robust and multi-purpose dialog systems using fewer data points and less annotation. She also works on language generation, vision and language tasks. Zhou&#8217;s work on persuasive dialog systems received an ACL 2019 best paper nomination recently. Zhou was featured in Forbes as 2018 30 under 30 in Science for her work on multimodal dialog systems. Her team recently won the 2018 Amazon Alexa Prize on building an engaging social bot for a $500,000 cash award.</div></div><div class="clear"></div></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Nov 4</b></div></td>
  <td valign=top ><div class="aiml-name"><a href="https://gengji.net"><img src="http://cml.ics.uci.edu/wp-content/uploads/geng-ji-square.jpg" width=200px/><br><b>Geng Ji</b></a><br>PhD Student<br>Dept of Computer Science<br>University of California, Irvine</div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Variational Inference: To Derive or Not To Derive</b></a></span></div><div class="togglec clearfix">Variational inference provides a general optimization framework to approximate the posterior distributions of latent variables in probabilistic models. Although effective in simple scenarios, it may be inaccurate or infeasible when the data is high-dimensional, the model structure is complicated, or variable relationships are non-conjugate. In this talk, I will present two different strategies to solve these problems. The first one is to derive rigorous variational bounds by leveraging the probabilistic relations and structural dependencies of the given model. One example I will explore is large-scale noisy-OR Bayesian networks popular in IT companies for analyzing the semantic content of massive text datasets. The second strategy is to create flexible algorithms directly applicable to many models, as can be expressed by probabilistic programming systems. I’ll talk about a low-variance Monte Carlo variational inference framework we recently developed for arbitrary models with discrete variables. It has appealing advantages over REINFORCE-style stochastic gradient estimates and model-dependent auxiliary-variable solutions, as demonstrated on real-world models of images, text, and social networks.
<br><br>
<b>Bio:</b> Geng Ji is a PhD candidate in the CS Department of UC Irvine, advised by Professor Erik Sudderth. His research interests are broadly in probabilistic graphical models, large-scale variational inference, as well as their applications in computer vision and natural language processing. He did summer internships at Disney Research in 2017 mentored by Professor Stephan Mandt, and Facebook AI in 2018 which he will join as a full-time research scientist.</div></div><div class="clear"></div>
</div>
  </td>
  </tr>

  <tr>
  <td valign=top  class='aiml-none'><div class="aiml-date"><b>Nov 11</b></div></td>
  <td valign=top  class='aiml-none'><div class="aiml-name"><b>Veterans Day</b></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Nov 18</b><br>4011<br>Bren Hall<br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://jthalloran.bitbucket.io/"><img src="http://cml.ics.uci.edu/wp-content/uploads/headShot.jpg" width=200px/><br><b>John T. Halloran</b></a><br>Postdoctoral Researcher<br>Dept. of Biomedical Engineering<br>University of California, Davis</div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Accelerated Machine Learning for Computational Proteomics</b></a></span></div><div class="togglec clearfix">In the past few decades, mass spectrometry-based proteomics has
dramatically improved our fundamental knowledge of biology, leading to
advancements in the understanding of diseases and methods for clinical
diagnoses.  However, the complexity and sheer volume of typical
proteomics datasets make both fast and accurate analysis difficult to
accomplish simultaneously; while machine learning
methods have proven themselves capable of incredibly accurate
proteomic analysis, such methods deter use by requiring extremely long
runtimes in practice.  In this talk, we will discuss two core problems in
computational proteomics and how to accelerate the training of
their highly accurate, but slow, machine learning solutions.  For the
first problem, wherein we seek to infer the protein subsequences
(called peptides) present in a biological sample, we will improve the
training of graphical models by deriving emission functions which
render conditional-maximum likelihood learning concave.  Used within a
dynamic Bayesian network, we show that these emission functions not
only allow extremely efficient learning of globally-convergent parameters,
but also drastically outperform the state-of-the-art in peptide
identification accuracy.  For the second problem, wherein we
seek to further improve peptide identification accuracy by
classifying correct versus incorrect identifications, we will
speed up the state-of-the-art in discriminative learning using a
combination of improved convex optimization and extensive
parallelization.  We show that on massive datasets containing
hundreds-of-millions of peptide identifications, these speedups reduce
discriminative analysis time from several days down to just several
hours, without any degradation in analysis quality.
<br>
<br>
<b>Bio:</b> John Halloran is a Postdoc at UC Davis working with Professor David Rocke.  He received his PhD from the University of Washington in 2016.  John is interested in developing fast and accurate machine learning solutions for massive-scale problems encountered in computational biology.  His work regularly focuses on efficient generative and discriminative training of dynamic graphical models. He is a recipient of the UC Davis Award for Excellence in Postdoctoral Research and a UW Genome Training Grant.</div></div><div class="clear"></div></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Nov 25</b><br>4011<br>Bren Hall<br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://www.cs.hmc.edu/~xanda/#/"><img src="http://cml.ics.uci.edu/wp-content/uploads/image.png" width=200px/><br><b>Xanda Schofield</b></a><br>Assistant Professor<br>Dept. of Computer Science<br>Harvey Mudd College</div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Towards Practical and Locally Private Topic Models</b></a></span></div><div class="togglec clearfix">A critical challenge in the large-scale analysis of people&#8217;s data is protecting the privacy of the people who generated it. Of particular interest is how to privately infer models over discrete count data, like frequencies of words in a message or the number of times two people have interacted. Recently, I helped to develop locally private Bayesian Poisson factorization, a method for differentially private inference for a large family of models of count data, including topic models, stochastic block models, event models, and beyond. However, in the domain of topic models over text, this method can encounter serious obstacles in both speed and model quality. These arise from the collision of high-dimensional, sparse counts of text features in a bag-of-words representation, and dense noise from a privacy mechanism. In this talk, I address several challenges in the space of private statistical model inference over language data, as well as corresponding approaches to produce interpretable models.
<br><br>
<b>Bio:</b> Xanda Schofield is an Assistant Professor in Computer Science at Harvey Mudd College. Her work focuses on practical applications of unsupervised models of text, particularly topic models, to research in the humanities and social sciences. More recently, her work has expanded to the intersection of privacy and text mining. She completed her Ph.D. in 2019 at Cornell University advised by David Mimno. In her graduate career, she was the recipient of an NDSEG Fellowship, the Anita Borg Memorial Scholarship, and the Microsoft Graduate Women&#8217;s Scholarship. She is also an avid cookie baker and tweets <a href="https://twitter.com/XandaSchofield">@XandaSchofield</a>.</div></div><div class="clear"></div></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Dec 2</b><br>4011<br>Bren Hall<br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://faculty.sites.uci.edu/doroudis/"><img src="http://cml.ics.uci.edu/wp-content/uploads/F03258CF-B843-4597-BE1E-4119C502282A.png" width=200px/><br><b>Shayan Doroudi</b></a><br>Assistant Professor<br>School of Education<br>University of California, Irvine</div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Bias, Variance, and the Intertwined Histories of Artificial Intelligence and Education Research</b></a></span></div><div class="togglec clearfix">This talk will be divided into two parts. In the first part, I will demonstrate that the bias-variance tradeoff in machine learning and statistics can be generalized to offer insights to debates in other scientific fields. In particular, I will show how it can be applied to situate a variety of debates that appear in the education literature. In the second part of my talk, I will give a brief account of how the early history of artificial intelligence was naturally intertwined with the history of education research and the learning sciences. I will use the generalized bias-variance tradeoff as a lens with which to situate different trends that appeared in this history. Today, AI researchers might see education as just another application area, but historically AI and education were integrated into a broader movement to understand and improve intelligence and learning, in humans and in machines.
<br><br>
<b>Bio:</b> Shayan Doroudi is an assistant professor at the UC Irvine School of Education. His research is focused on the learning sciences, educational technology, and the educational data sciences. He is particularly interested in studying the prospects and limitations of data-driven algorithms in learning technologies, including lessons that can be drawn from the rich history of educational technology. He earned his B.S. in Computer Science from the California Institute of Technology, and his M.S. and Ph.D. in Computer Science from Carnegie Mellon.</div></div><div class="clear"></div></div>
  </td>
  </tr>

  <tr>
  <td valign=top  class='aiml-none'><div class="aiml-date"><b>Dec 9</b></div></td>
  <td valign=top  class='aiml-none'><div class="aiml-name"><b>Finals week</b></div>
  </td>
  </tr>

  <tr>
  <td valign=top ><div class="aiml-date"><b>Dec 16</b><br>4011<br>Bren Hall<br>1 pm</div></td>
  <td valign=top ><div class="aiml-name"><a href="https://enalisnick.github.io/"><img src="http://cml.ics.uci.edu/wp-content/uploads/headshot2.jpg" width=200px/><br><b>Eric Nalisnick</b></a><br>Postdoctoral Researcher<br>University of Cambridge/DeepMind</div><br> <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Deep Learning Under Covariate Shift</b></a></span></div><div class="togglec clearfix">Deep neural networks have demonstrated impressive performance
in predictive tasks.  However, these models have been shown to be
brittle, being easily fooled by even small perturbations of the input
features (covariates).  In this talk, I describe two approaches for
handling covariate shift.  The first uses a Bayesian prior derived from
data augmentation to make the classifier robust to potential test-time
shifts.  The second strategy is to directly model the covariates using a
&#8216;hybrid model&#8217;: a model of the joint distribution over labels and
features. In experiments involving this latter approach, we discovered
limitations in some existing methods for detecting distributional shift
in high-dimensions.  I demonstrate that a simple entropy-based
goodness-of-fit test can solve some of these issues but conclude by
arguing that more investigation is needed.
<br><br>
<b>Bio:</b> Eric Nalisnick is a postdoctoral researcher at the University of
Cambridge and a part-time research scientist at DeepMind.  His research
interests span statistical machine learning, with a current emphasis on
Bayesian deep learning, generative modeling, and out-of-distribution
detection.  He received his PhD from the University of California,
Irvine, where he was supervised by Padhraic Smyth. Eric has also spent
time interning at DeepMind, Twitter, Microsoft, and Amazon.</div></div><div class="clear"></div></div>
  </td>
  </tr>

</tbody></table>



<p></p>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2019/09/fall-2019/" title="5:19 pm" rel="bookmark"><time class="entry-date genericon" datetime="2019-09-05T17:19:13-07:00">September 5, 2019</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
				
<article id="post-849" class="post-849 post type-post status-publish format-standard hentry category-aiml">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2019/04/spring-2019/" title="Permalink to Spring 2019" rel="bookmark">Spring 2019</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		
<!-- ========================================================== -->
<!-- =====Spring Quarter ============================================== -->
<!-- ========================================================== -->

   <table cellpadding=5 border=1>
   <col width="100"><col>

  <!-- ==== Apr 8 =================================== -->
  <tr>
  <td valign=top class='aiml-none'><div class="aiml-date"><b>Apr 8</b></div></td>
  <td valign=top class='aiml-none'><div class="aiml-name"><b>No Seminar</b><br></div><br>
  </td>
  </tr>



  <!-- ==== Apr 15 =================================== -->
  <tr>
  <td valign=top><div class="aiml-date"><b>Apr 15</b><br>Bren Hall 4011<br>1 pm</div></td>
  <td valign=top><div class="aiml-name"><a href="http://research.dshin.org/"><b>Daeyun Shin</b></a><br>PhD Candidate<br>Dept of Computer Science<br>UC Irvine</div><br>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Multi-layer Depth and Epipolar Feature Transformers for 3D Scene Reconstruction</b></a></span></div><div class="togglec clearfix">In this presentation, I will present our approach to the problem of automatically reconstructing a complete 3D model of a scene from a single RGB image.  This challenging task requires inferring the shape of both visible and occluded surfaces.  Our approach utilizes viewer-centered, multi-layer representation of scene geometry adapted from recent methods for single object shape completion.  To improve the accuracy of view-centered representations for complex scenes, we introduce a novel “Epipolar Feature Transformer” that transfers convolutional network features from an input view to other virtual camera viewpoints, and thus better covers the 3D scene geometry.  Unlike existing approaches that first detect and localize objects in 3D, and then infer object shape using category-specific models, our approach is fully convolutional, end-to-end differentiable, and avoids the resolution and memory limitations of voxel representations.  We demonstrate the advantages of multi-layer depth representations and epipolar feature transformers on the reconstruction of a large database of indoor scenes.  <p><p>Project page: <a href=https://www.ics.uci.edu/~daeyuns/layered-epipolar-cnn/>https://www.ics.uci.edu/~daeyuns/layered-epipolar-cnn/</a></div></div><div class="clear"></div>
  </td>
  </tr>

  <!-- ==== Apr 22 =================================== -->
  <tr>
  <td valign=top><div class="aiml-date"><b>Apr 22</b><br>Bren Hall 4011<br>1 pm</div></td>
  <td valign=top><div class="aiml-name"><a href="http://sites.uci.edu/pritchard"><b>Mike Pritchard</b></a><br>Assistant Professor<br>Dept. of Earth System Sciences<br>University of California, Irvine</div><br>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Improving global climate simulations using physically constrained deep learning emulators of unresolved moist turbulence processes</b></a></span></div><div class="togglec clearfix">I will discuss machine-learning emulation of O(100M) cloud-resolving simulations of moist turbulence for use in multi-scale global climate simulation. First, I will present encouraging results from pilot tests on an idealized ocean-world, in which a fully connected deep neural network (DNN) is found to be capable of emulating explicit subgrid vertical heat and vapor transports across a globally diverse population of convective regimes. Next, I will demonstrate that O(10k) instances of the DNN emulator spanning the world are able to feed back realistically with a prognostic global host atmospheric model, producing viable ML-powered climate simulations that exhibit realistic space-time variability for convectively coupled weather dynamics and even some limited out-of-sample generalizability to new climate states beyond the training data’s boundaries. I will then discuss a new prototype of the neural network under development that includes the ability to enforce multiple physical constraints within the DNN optimization process, which exhibits potential for further generalizability. Finally, I will conclude with some discussion of the unsolved technical issues and interesting philosophical tensions being raised in the climate modeling community by this disruptive but promising approach for next-generation global simulation.</div></div><div class="clear"></div>
  </td>
  </tr>



  <!-- ==== Apr 29 =================================== -->
  <tr>
  <td valign=top><div class="aiml-date"><b>Apr 29</b><br>Bren Hall 4011<br>1 pm</div></td>
  <td valign=top><div class="aiml-name"><a href=""><b>Nick Gallo</b></a><br>PhD Candidate<br>Department of Computer Science<br>University of California, Irvine</div><br>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Coarse to Fine Lifted Inference</b></a></span></div><div class="togglec clearfix">Large problems with repetitive sub-structure arise in many domains such as social network analysis, collective classification, and database entity resolution.  In these instances, individual data is augmented with a small set of rules that uniformly govern the relationship among groups of objects (for example: &#8220;the friend of my friend is probably my friend&#8221; in a social network).  Uncertainty is captured by a probabilistic graphical model structure.  While theoretically sound, standard reasoning techniques cannot be applied due to the massive size of the network (often millions of random variable and trillions of factors).  Previous work on lifted inference efficiently exploits symmetric structure in graphical models, but breaks down in the presence of unique individual data (contained in all real-world problems).  Current methods to address this problem are largely heuristic.  In this presentation we describe a coarse to fine approximate inference framework that initially treats all individuals identically, gradually relaxing this restriction to finer sub-groups.  This produces a sequence of inference objective bounds of monotonically increasing cost and accuracy.  We then discuss our work on incorporating high-order inference terms (over large subsets of variables) into lifted inference and ongoing challenges in this area.</div></div><div class="clear"></div>
  </td>
  </tr>


  <!-- ==== May 13 =================================== -->
  <tr>
  <td valign=top><div class="aiml-date"><b>May 13</b><br>Bren Hall 4011<br>1 pm</div></td>
  <td valign=top><div class="aiml-name"><a href="https://allenai.org/team/mattg/"><b>Matt Gardner</b></a><br>Senior Research Scientist<br>Allen Institute of Artificial Intelligence<br></div><br>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Reasoning Our Way to Reading</b></a></span></div><div class="togglec clearfix">Reading machines that truly understood what they read would change the world, but our current best reading systems struggle to understand text at anything more than a superficial level.  In this talk I try to reason out what it means to &#8220;read&#8221;, and how reasoning systems might help us get there.  I will introduce three reading comprehension datasets that require systems to reason at a deeper level about the text that they read, using numerical, coreferential, and implicative reasoning abilities.  I will also describe some early work on models that can perform these kinds of reasoning.  <p><p><b>Bio:</b> Matt is a senior research scientist at the Allen Institute for Artificial Intelligence (AI2) on the AllenNLP team, and a visiting scholar at UCI. His research focuses primarily on getting computers to read and answer questions, dealing both with open domain reading comprehension and with understanding question semantics in terms of some formal grounding (semantic parsing). He is particularly interested in cases where these two problems intersect, doing some kind of reasoning over open domain text. He is the original author of the AllenNLP toolkit for NLP research, and he co-hosts the NLP Highlights podcast with Waleed Ammar.</div></div><div class="clear"></div>
  </td>
  </tr>

  <!-- ==== May 27 =================================== -->
  <tr>
  <td valign=top class='aiml-none'><div class="aiml-date"><b>May 27</b></div></td>
  <td valign=top class='aiml-none'><div class="aiml-name"><b>No Seminar (Memorial Day)</b><br></div><br>
  </td>
  </tr>


  <!-- ==== June 3 =================================== -->
  <tr>
  <td valign=top><div class="aiml-date"><b>June 3</b><br>Bren Hall 4011<br>12:00</div></td>
  <td valign=top><div class="aiml-name"><a href="www2.hawaii.edu/~psadow"><b>Peter Sadowski</b></a><br>Assistant Professor<br>Information and Computer Sciences<br>University of Hawaii Manoa</div><br>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Deep Learning for Extreme Remote Sensing: from Ocean Waves to Exocomets</b></a></span></div><div class="togglec clearfix">New technologies for remote sensing and astronomy provide an unprecedented view of Earth, our Sun, and beyond. Traditional data-analysis pipelines in oceanography, atmospheric sciences, and astronomy struggle to take full advantage of the massive amounts of high-dimensional data now available. I will describe opportunities for using deep learning to process satellite and telescope data, and discuss recent work mapping extreme sea states using Satellite Aperture Radar (SAR), inferring the physics of our sun&#8217;s atmosphere, and detecting anomalous astrophysical events in other systems, such as comets transiting distant stars.<p><p><b>Bio:</b> Peter Sadowski is an Assistant Professor of Information and Computer Sciences at the University of Hawaii Manoa and Co-Director of the AI Precision Health Institute at the University of Hawaii Cancer Center. He completed his Ph.D. and Postdoc at University of California Irvine, and his undergraduate studies at  Caltech. His research focuses on deep learning and its applications to the natural sciences, particularly those at the intersection of machine learning and physics.</div></div><div class="clear"></div>
  </td>
  </tr>



  <!-- ==== June 3 =================================== -->
  <tr>
  <td valign=top><div class="aiml-date"><b>June 3</b><br>Bren Hall 4011<br>1 pm</div></td>
  <td valign=top><div class="aiml-name"><a href="https://staff.fnwi.uva.nl/m.welling/"><b>Max Welling</b></a><br>Research Chair, University of Amsterdam<br>VP Technologies, Qualcomm<br></div><br>
  <div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Integrating Generative Modeling into Deep Learning</b></a></span></div><div class="togglec clearfix">Deep learning has boosted the performance of many applications tremendously, such as object classification and detection in images, speech recognition and understanding, machine translation, game play such as chess and go etc. However, these all constitute reasonably narrowly and well defined tasks for which it is reasonable to collect very large datasets. For artificial general intelligence (AGI) we will need to learn from a small number of samples, generalize to entirely new domains, and reason about a problem. What do we need in order to make progress to AGI? I will argue that we need to combine the data generating process, such as the physics of the domain and the causal relationships between objects, with the tools of deep learning. In this talk I will present a first attempt to integrate the theory of graphical models, which arguably was the dominating modeling machine learning paradigm around the turn of the twenty-first century, with deep learning. Graphical models express the relations between random variables in an interpretable way, while probabilistic inference in such networks can be used to reason about these variables. We will propose a new hybrid paradigm where probabilistic message passing in such networks is enhanced with graph convolutional neural networks to improve the ability of such systems to reason and make predictions.</div></div><div class="clear"></div>
  </td>
  </tr>

  <!-- ==== June 10 =================================== -->
  <tr>
  <td valign=top class='aiml-none'><div class="aiml-date"><b>June 10</b></div></td>
  <td valign=top class='aiml-none'><div class="aiml-name"><b>No Seminar (Finals)</b><br></div><br>
  </td>
  </tr>


</table>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2019/04/spring-2019/" title="5:12 pm" rel="bookmark"><time class="entry-date genericon" datetime="2019-04-11T17:12:02-07:00">April 11, 2019</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
				
<article id="post-811" class="post-811 post type-post status-publish format-standard hentry category-aiml">
	<header class="entry-header">
		<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->
		<!--<div class="entry-thumbnail">
					</div> -->
		<h1 class="entry-title"><a href="https://cml.ics.uci.edu/2018/09/fall-2018/" title="Permalink to Fall 2018" rel="bookmark">Fall 2018</a></h1>				<span class="entry-format genericon">Standard</span>			</header><!-- .entry-header -->

		<div class="entry-content">
		<p><!-- ========================================================== --><br />
<!-- =====Fall Quarter ============================================== --><br />
<!-- ========================================================== --></p>
<table border="1" cellpadding="5"><!-- ==== Oct 1 =================================== --></p>
<tbody>
<tr>
<td class="aiml-none" valign="top">
<div class="aiml-date"><b>Oct 1</b></div>
</td>
<td class="aiml-none" valign="top">
<div class="aiml-name"><b>No Seminar</b></div>
<p>&nbsp;</td>
</tr>
<p><!-- ==== Oct 8 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>Oct 8</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><a href="https://allenai.org/team/mattg/"><b>Matt Gardner</b></a><br />
Research Scientist<br />
Allen Institute for AI</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>A Tale of Two Question Answering Systems</b></a></span></div><div class="togglec clearfix">The path to natural language understanding goes through increasingly challenging question answering tasks. I will present research that significantly improves performance on two such tasks: answering complex questions over tables, and open-domain factoid question answering. For answering complex questions, I will present a type-constrained encoder-decoder neural semantic parser that learns to map natural language questions to programs. For open-domain factoid QA, I will show that training paragraph-level QA systems to give calibrated confidence scores across paragraphs is crucial when the correct answer-containing paragraph is unknown. I will conclude with some thoughts about how to combine these two disparate QA paradigms, towards the goal of answering complex questions over open-domain text.</p>
<p><b>Bio:</b>Matt Gardner is a research scientist at the Allen Institute for Artificial Intelligence (AI2), where he has been exploring various kinds of question answering systems. He is the lead designer and maintainer of the AllenNLP toolkit, a platform for doing NLP research on top of pytorch. Matt is also the co-host of the NLP Highlights podcast, where, with Waleed Ammar, he gets to interview the authors of interesting NLP papers about their work. Prior to joining AI2, Matt earned a PhD from Carnegie Mellon University, working with Tom Mitchell on the Never Ending Language Learning project.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Oct 15 =================================== --></p>
<p><!-- ==== Oct 22 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>Oct 22</b></div>
<div>
<div></div>
</div>
<div>Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><a style="font-family: inherit; font-size: inherit;" href="http://www.stephanmandt.com/"><b>Stephan Mandt</b></a></div>
</div>
<div class="aiml-name">
<div class="aiml-name">Assistant Professor<br />
Dept. of Computer Science<br />
UC Irvine</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Deep Probabilistic Modeling</b></a></span></div><div class="togglec clearfix">I will give an overview of some exciting recent developments in deep probabilistic modeling, which combines deep neural networks with probabilistic models for unsupervised learning. Deep probabilistic models are capable of synthesizing artificial data that highly resemble the training data, and are able fool both machine learning classifiers as well as humans. These models have numerous applications in creative tasks, such as voice, image, or video synthesis and manipulation. At the same time, combining neural networks with strong priors results in flexible yet highly interpretable models for finding hidden structure in large data sets. I will summarize my group’s activities in this space, including measuring semantic shifts of individual words over hundreds of years, summarizing audience reactions to movies, and predicting the future evolution of video sequences with applications to neural video coding.</div></div><div class="clear"></div>
</div>
</tr>
<p><!-- ==== Oct 25 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>Oct 25</b><br />
Bren Hall 3011<br />
3 pm</div>
</td>
<td valign="top">
<div class="aiml-name">
<div class="aiml-name">
<div class="aiml-name"></div>
<div></div>
</div>
</div>
<div class="aiml-name">
<div><strong>(Note: different day (Thurs), time (3pm), and location (3011) relative to usual Monday seminars)</p>
<p></strong></div>
<div><a href="http://pages.cs.wisc.edu/~swright/"><b>Steven Wright</b></a><br />
Professor<br />
Department of Computer Sciences<br />
University of Wisconsin, Madison</div>
</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Optimization in Data Science</b></a></span></div><div class="togglec clearfix">Many of the computational problems that arise in data analysis and<br />
machine learning can be expressed mathematically as <span class="m_-567682425052202021gmail-il">optimization </span>problems. Indeed, much new algorithmic research in <span class="m_-567682425052202021gmail-il">optimization</span> is being driven by the need to solve large, complex problems from these areas. In this talk, we review a number of canonical problems in data analysis and their formulations as <span class="m_-567682425052202021gmail-il">optimization</span> problems. We will cover support vector machines / kernel learning, logistic regression (including regularized and multiclass variants), matrix completion, deep learning, and several other paradigms.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Oct 29 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>Oct 29</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><a href="http://www.cs.cmu.edu/~cpsomas/"><b>Alex Psomas</b></a><br />
Postdoctoral Researcher<br />
Computer Science Department<br />
Carnegie Mellon University</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Fair Resource Allocation: From Theory to Practice</b></a></span></div><div class="togglec clearfix">We study the problem of fairly allocating a set of indivisible items among $n$ agents. Typically, the literature has focused on one-shot algorithms. In this talk we depart from this paradigm and allow items to arrive online. When an item arrives we must immediately and irrevocably allocate it to an agent. A paradigmatic example is that of food banks: food donations arrive, and must be delivered to nonprofit organizations such as food pantries and soup kitchens. Items are often perishable, which is why allocation decisions must be made quickly, and donated items are typically leftovers, leading to lack of information about items that will arrive in the future. Which recipient should a new donation go to? We approach this problem from different angles.</p>
<p>In the first part of the talk, we study the problem of minimizing the maximum envy between any two recipients, after all the goods have been allocated. We give a polynomial-time, deterministic and asymptotically optimal algorithm with vanishing envy, i.e. the maximum envy divided by the number of items T goes to zero as T goes to infinity. In the second part of the talk, we adopt and further develop an emerging paradigm called virtual democracy. We will take these ideas all the way to practice. In the last part of the talk I will present some results from an ongoing work on automating the decisions faced by a food bank called 412 Food Rescue, an organization in Pittsburgh that matches food donations with non-profit organizations.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Nov 5 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>Nov 5</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><a href="http://www.fredpark.com/"><b>Fred Park</b></a><br />
Associate Professor<br />
Dept of Math &amp; Computer Science<br />
Whittier College</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Image Segmentation and Tracking Utilizing a Difference of Convex Regularized Mumford-Shah Functional</b></a></span></div><div class="togglec clearfix">In this talk I will give a brief overview of the segmentation and tracking problems and will propose a new model that tackles both of them. This model incorporates a weighted difference of anisotropic and isotropic total variation (TV) norms into a relaxed formulation of the Mumford-Shah (MS) model. We will show results exceeding those obtained by the MS model when using the standard TV norm to regularize partition boundaries. Examples illustrating the qualitative differences between the proposed model and the standard MS one will be shown as well. I will also talk about a fast numerical method that is used to optimize the proposed model utilizing the difference-of-convex algorithm (DCA) and the primal dual hybrid gradient (PDHG) method. Finally, future directions will be given that could harness the power of convolution nets for more advanced segmentation tasks.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Nov 12 =================================== --></p>
<tr>
<td class="aiml-none" valign="top">
<div class="aiml-date"><b>Nov 12</b></div>
</td>
<td class="aiml-none" valign="top">
<div class="aiml-name"><b>No Seminar (Veterans Day)</b></div>
<p>&nbsp;</td>
</tr>
<p><!-- ==== Nov 19 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>Nov 19</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><a href="https://ai.google/research/people/PhilipNelson"><b>Philip Nelson</b></a><br />
Director of Engineering<br />
Google Research</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Accelerating bio discovery with machine learning, the promise and the peril</b></a></span></div><div class="togglec clearfix">Google Accelerated Sciences is a translational research team that brings Google&#8217;s technological expertise to the scientific community.  Recent advances in machine learning have delivered incredible results in consumer applications (e.g. photo recognition, language translation), and is now beginning to play an important role in life sciences.  Taking examples from active collaborations in the biochemical, biological, and biomedical fields, I will focus on how our team transforms science problems into data problems and applies Google&#8217;s scaled computation, data-driven engineering, and machine learning to accelerate discovery. See <a href="http://g.co/research/gas">http://g.co/research/gas</a> for our publications and more details.</p>
<p><b>Bio:</b><br />Philip Nelson is a Director of Engineering in Google Research.  He joined Google in 2008 and was previously responsible for a range of Google applications and geo services.  In 2013, he helped found and currently leads the Google Accelerated Science team that collaborates with academic and commercial scientists to apply Google&#8217;s knowledge and experience and technologies to important scientific problems.  Philip graduated from MIT in 1985 where he did award-winning research on hip prosthetics at Harvard Medical School.  Before Google, Philip helped found and lead several Silicon Valley startups in search (Verity), optimization (Impresse), and genome sequencing (Complete Genomics) and was also an Entrepreneur in Residence at Accel Partners.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Nov 26 =================================== --></p>
<tr>
<td valign="top">
<div class="aiml-date"><b>Nov 26</b><br />
Bren Hall 4011<br />
1 pm</div>
</td>
<td valign="top">
<div class="aiml-name"><a href="http://socsci.uci.edu/~rfutrell/"><b>Richard Futrell</b></a><br />
Assistant Professor<br />
Dept of Language Science<br />
UC Irvine</div>
<p><div class="toggle clearfix wp_shortcodes_toggle"><div class="wps_togglet"><span><a><b>Natural language as a code: Modeling human language using information theory</b></a></span></div><div class="togglec clearfix"><br />
Why is natural language the way it is? I propose that human languages can be modeled as solutions to the problem of efficient communication among intelligent agents with certain information processing constraints, in particular constraints on short-term memory. I present an analysis of dependency treebank corpora of over 50 languages showing that word orders across languages are optimized to limit short-term memory demands in parsing. Next I develop a Bayesian, information-theoretic model of human language processing, and show that this model can intuitively explain an apparently paradoxical class of comprehension errors made by both humans and state-of-the-art recurrent neural networks (RNNs). Finally I combine these insights in a model of human languages as information-theoretic codes for latent tree structures, and show that optimization of these codes for expressivity and compressibility results in grammars that resemble human languages.</div></div><div class="clear"></div></td>
</tr>
<p><!-- ==== Dec 3 =================================== --></p>
<tr>
<td class="aiml-none" valign="top">
<div class="aiml-date"><b>Dec 3</b></div>
</td>
<td class="aiml-none" valign="top">
<div class="aiml-name"><b>No Seminar (NIPS)</b></div>
<p>&nbsp;</td>
</tr>
</tbody>
</table>
			</div><!-- .entry-content -->
	
	<footer class="entry-meta">
				<a href="https://cml.ics.uci.edu/2018/09/fall-2018/" title="2:24 pm" rel="bookmark"><time class="entry-date genericon" datetime="2018-09-18T14:24:22-07:00">September 18, 2018</time></a>		
		
			</footer><!-- .entry-meta -->
</article><!-- #post-## -->


			
				<nav class="navigation-paging" role="navigation">
		<h1 class="screen-reader-text">Posts navigation</h1>
		<div class="nav-links">

						<div class="nav-previous"><a href="https://cml.ics.uci.edu/category/aiml/page/2/" ><span class="meta-nav">&lsaquo;</span><span class="screen-reader-text">Older posts</span></a></div>
			
			
		</div><!-- .nav-links -->
	</nav><!-- .navigation -->
	
		
		</div><!-- #content -->
	</section><!-- #primary -->

	<div id="secondary" class="widget-area" role="complementary">
				<aside id="search-2" class="widget widget_search">	<form method="get" id="searchform" class="searchform" action="https://cml.ics.uci.edu/" role="search">
		<label for="s" class="screen-reader-text">Search</label>
		<input type="search" class="field" name="s" value="" id="s" placeholder="Search &hellip;" />
		<input type="submit" class="submit" id="searchsubmit" value="Search" />
	</form>
</aside>	</div><!-- #secondary -->

</div><!-- #page -->

<footer id="colophon" class="site-footer" role="contentinfo">
<p style="text-align:center;margin:0;">(c) 2015 <a href="http://cml.ics.uci.edu">Center for Machine Learning and Intelligent Systems</a>
	<div class="site-info">
				<a href="http://wordpress.org/" rel="generator">WordPress</a>/<a href="http://www.wpzoom.com/">BonPress</a>
	</div><!-- .site-info -->
</footer><!-- #colophon -->

<script type='text/javascript' src='https://cml.ics.uci.edu/wp-content/themes/bonpress-wpcom/js/navigation.js?ver=20120206' id='bonpress-navigation-js'></script>
<script type='text/javascript' src='https://cml.ics.uci.edu/wp-content/themes/bonpress-wpcom/js/skip-link-focus-fix.js?ver=20130115' id='bonpress-skip-link-focus-fix-js'></script>

</body>
</html>